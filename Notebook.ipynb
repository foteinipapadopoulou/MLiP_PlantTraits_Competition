{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":65626,"databundleVersionId":8046133,"sourceType":"competition"},{"sourceId":8145845,"sourceType":"datasetVersion","datasetId":4816952},{"sourceId":8149784,"sourceType":"datasetVersion","datasetId":4819871},{"sourceId":8178456,"sourceType":"datasetVersion","datasetId":4841404},{"sourceId":8204134,"sourceType":"datasetVersion","datasetId":4860734},{"sourceId":8273592,"sourceType":"datasetVersion","datasetId":4912621},{"sourceId":8332370,"sourceType":"datasetVersion","datasetId":4862895},{"sourceId":8332380,"sourceType":"datasetVersion","datasetId":4947895}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"- Notebook modified from https://www.kaggle.com/code/markwijkhuizen/planttraits2024-eda-training-pub.\n- Training only, EDA part not included.\n- Image model only, tabular data not used.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport imageio.v3 as imageio\nimport albumentations as A\n\nfrom albumentations.pytorch import ToTensorV2\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch import nn\nfrom tqdm.notebook import tqdm\nfrom sklearn.preprocessing import StandardScaler\n\nimport torch\nimport timm\nimport glob\nimport torchmetrics\nimport time\nimport psutil\nimport os\n\ntqdm.pandas()","metadata":{"execution":{"iopub.status.busy":"2024-05-12T21:20:34.622773Z","iopub.execute_input":"2024-05-12T21:20:34.623082Z","iopub.status.idle":"2024-05-12T21:20:44.472710Z","shell.execute_reply.started":"2024-05-12T21:20:34.623057Z","shell.execute_reply":"2024-05-12T21:20:44.471745Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"class Config():\n    IMAGE_SIZE = 384#196#224\n    BACKBONE = 'swin_large_patch4_window12_384.ms_in22k_ft_in1k'#'eva_large_patch14_196.in22k_ft_in22k_in1k'#'vit_base_patch16_clip_224.openai_ft_in12k'#'efficientnet_b0' #\n    TARGET_COLUMNS = ['X4_mean', 'X11_mean', 'X18_mean', 'X50_mean', 'X26_mean', 'X3112_mean']\n    N_TARGETS = len(TARGET_COLUMNS)\n    BATCH_SIZE = 10\n    LR_MAX = 1e-4\n    WEIGHT_DECAY = 0.01\n    N_EPOCHS = 6\n    TRAIN_MODEL = True\n    IS_INTERACTIVE = os.environ['KAGGLE_KERNEL_RUN_TYPE'] == 'Interactive'\n    BUILD_PKL_DATASET = False\n    BUILD_VALID_SET = False\n    USE_SMALL_DATASET = False\n    USE_VALID_SET = False\n    USE_MODIFIED_TRAIN = False\n    USE_XGBOOST = False\n    USE_PREPROCESSING = False\n    IMAGES_OUTPUT_DIM = 128\n    TABULAR_OUTPUT_DIM = 128\n    TABULAR_INPUT_DIM = 128\n        \nCONFIG = Config()","metadata":{"execution":{"iopub.status.busy":"2024-05-12T21:24:04.484151Z","iopub.execute_input":"2024-05-12T21:24:04.484858Z","iopub.status.idle":"2024-05-12T21:24:04.491626Z","shell.execute_reply.started":"2024-05-12T21:24:04.484829Z","shell.execute_reply":"2024-05-12T21:24:04.490573Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"if CONFIG.BUILD_PKL_DATASET is True:\n    train = pd.read_csv('/kaggle/input/planttraits2024/train.csv')\n    train['file_path'] = train['id'].apply(lambda s: f'/kaggle/input/planttraits2024/train_images/{s}.jpeg')\n    train['jpeg_bytes'] = train['file_path'].progress_apply(lambda fp: open(fp, 'rb').read())\n    train.to_pickle('train.pkl')\n    test = pd.read_csv('/kaggle/input/planttraits2024/test.csv')\n    test['file_path'] = test['id'].apply(lambda s: f'/kaggle/input/planttraits2024/test_images/{s}.jpeg')\n    test['jpeg_bytes'] = test['file_path'].progress_apply(lambda fp: open(fp, 'rb').read())\n    test.to_pickle('test.pkl')\nelse:\n    if CONFIG.USE_SMALL_DATASET is True:\n        train = pd.read_pickle('/kaggle/input/dataset-with-validation/small_train.pkl')       \n    elif CONFIG.USE_MODIFIED_TRAIN is True:\n        train = pd.read_pickle('/kaggle/input/dataset-with-validation/train_set.pkl')\n    else:\n         train = pd.read_pickle('/kaggle/input/baseline-model/train.pkl')\n    test = pd.read_pickle('/kaggle/input/baseline-model/test.pkl')\n    \nfor column in CONFIG.TARGET_COLUMNS:\n    lower_quantile = train[column].quantile(0.005)\n    upper_quantile = train[column].quantile(0.985)  \n    train = train[(train[column] >= lower_quantile) & (train[column] <= upper_quantile)]\n\nCONFIG.N_TRAIN_SAMPLES = len(train)\nCONFIG.N_STEPS_PER_EPOCH = (CONFIG.N_TRAIN_SAMPLES // CONFIG.BATCH_SIZE)\nCONFIG.N_STEPS = CONFIG.N_STEPS_PER_EPOCH * CONFIG.N_EPOCHS + 1\n\nprint('N_TRAIN_SAMPLES:', len(train), 'N_TEST_SAMPLES:', len(test))","metadata":{"execution":{"iopub.status.busy":"2024-05-12T21:24:04.975918Z","iopub.execute_input":"2024-05-12T21:24:04.976247Z","iopub.status.idle":"2024-05-12T21:24:08.292904Z","shell.execute_reply.started":"2024-05-12T21:24:04.976223Z","shell.execute_reply":"2024-05-12T21:24:08.291922Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"N_TRAIN_SAMPLES: 49168 N_TEST_SAMPLES: 6545\n","output_type":"stream"}]},{"cell_type":"code","source":"#all columns must be identical to be consider the same species\ntrait_columns = ['X4_mean', 'X11_mean', 'X18_mean', 'X50_mean', 'X26_mean', 'X3112_mean']\ntrain['species'] = train.groupby(CONFIG.TARGET_COLUMNS).ngroup()\nspecies_counts = train['species'].nunique()\nprint (f\"{species_counts} unique species found in {len(train)} records\")","metadata":{"execution":{"iopub.status.busy":"2024-05-12T21:24:08.294364Z","iopub.execute_input":"2024-05-12T21:24:08.294667Z","iopub.status.idle":"2024-05-12T21:24:08.328246Z","shell.execute_reply.started":"2024-05-12T21:24:08.294642Z","shell.execute_reply":"2024-05-12T21:24:08.327359Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"17535 unique species found in 49168 records\n","output_type":"stream"}]},{"cell_type":"code","source":"species_counts = train['species'].value_counts()\ncount_summary = species_counts.value_counts().sort_index()\ncount_summary.plot(kind='bar', figsize=(7, 3))\n\nplt.title('Images per Species')\nplt.xlabel('Images')\nplt.ylabel('Species')\nplt.tight_layout() \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-12T21:24:08.329262Z","iopub.execute_input":"2024-05-12T21:24:08.329554Z","iopub.status.idle":"2024-05-12T21:24:08.685235Z","shell.execute_reply.started":"2024-05-12T21:24:08.329513Z","shell.execute_reply":"2024-05-12T21:24:08.684338Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 700x300 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"markdown","source":"## Preprocessing","metadata":{"execution":{"iopub.status.busy":"2024-04-29T15:47:23.898819Z","iopub.execute_input":"2024-04-29T15:47:23.900126Z","iopub.status.idle":"2024-04-29T15:47:23.904603Z","shell.execute_reply.started":"2024-04-29T15:47:23.900083Z","shell.execute_reply":"2024-04-29T15:47:23.903274Z"}}},{"cell_type":"markdown","source":"In the nature article, the authors performed data notmalisation in all the columns. In their case they had less auxilary data. Here, we will preprocess all the data.\n\nDetails for all these can be foud at https://www.kaggle.com/code/vaggelisspi/planttraits-working-through-the-nature-article#Pre-processing.","metadata":{}},{"cell_type":"code","source":"if CONFIG.USE_PREPROCESSING is True:\n    # not worring about '_sd' columns for now\n    sd_columns = [col for col in train.columns if col.endswith('_sd')]\n    train = train.drop(columns=sd_columns)\n    # the columns of the auxilary data\n    PREPROCESS_COLS = test.columns[1:-2].tolist()\n    # preprocess the data\n    norm_data = np.zeros_like(train[PREPROCESS_COLS], dtype=np.float32)\n    for col_idx, col in enumerate(PREPROCESS_COLS):\n        v = train[col].values\n        v = np.log10(v)\n        norm_data[:, col_idx] = v\n    # \n    norm_data[norm_data == np.inf] = 0\n    norm_data[norm_data == -np.inf] = 0\n    scaler = StandardScaler()\n    norm_data = scaler.fit_transform(norm_data)\n\n    for col_idx, col in enumerate(PREPROCESS_COLS):\n        v = norm_data[:, col_idx]\n        train[col] = v\n\n    train = train.fillna(0)\n    print(train.head())\n    print(train.isnull().sum().sum())\n    \n    # preprocess the test data\n    norm_data = np.zeros_like(test[PREPROCESS_COLS], dtype=np.float32)\n    for col_idx, col in enumerate(PREPROCESS_COLS):\n        v = test[col].values\n        v = np.log10(v)\n        norm_data[:, col_idx] = v\n\n    norm_data[norm_data == np.inf] = 0\n    norm_data[norm_data == -np.inf] = 0\n    # scaler = StandardScaler()\n    norm_data = scaler.fit_transform(norm_data)\n\n    for col_idx, col in enumerate(PREPROCESS_COLS):\n        v = norm_data[:, col_idx]\n        test[col] = v\n\n    print(test.head())\n    print(test.isnull().sum().sum())","metadata":{"execution":{"iopub.status.busy":"2024-05-12T21:24:08.687504Z","iopub.execute_input":"2024-05-12T21:24:08.687872Z","iopub.status.idle":"2024-05-12T21:24:08.699598Z","shell.execute_reply.started":"2024-05-12T21:24:08.687842Z","shell.execute_reply":"2024-05-12T21:24:08.698698Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"if CONFIG.BUILD_VALID_SET is True:\n    unique_species = train['species'].unique()\n    n_test_species_exclusive = 2\n\n    # Randomly pick exclusive test species\n    test_species_exclusive = np.random.choice(unique_species, n_test_species_exclusive, replace=False)\n\n    # Isolate exclusive test species data\n    exclusive_test_set = train[train['species'].isin(test_species_exclusive)]\n\n    # Remove exclusive test species from main data\n    train_reduced = train[~train['species'].isin(test_species_exclusive)]\n    print(len(train_reduced))\n    test_set_common = pd.DataFrame()\n    for species in train_reduced['species'].unique():\n        species_data = train_reduced[train_reduced['species'] == species]\n        sample = species_data.sample(1, random_state=42) \n        test_set_common = pd.concat([test_set_common, sample])\n\n    # Remove these samples from train\n    train_set = train_reduced.drop(test_set_common.index)\n\n    valid_set = pd.concat([exclusive_test_set, test_set_common])\n#     train_set.to_pickle('train_set.pkl')\n#     valid_set.to_pickle('valid_set.pkl')\n    \n    print(f\"Training set size: {len(train_set)}\")\n    print(f\"Valid set size: {len(final_test_set)}\")\n    print(f\"Unique species in Training set: {train_set['species'].nunique()}\")\n    print(f\"Unique species in Valid set: {final_test_set['species'].nunique()}\")\n    train = train_set\nelif CONFIG.USE_VALID_SET is True:\n    valid_set = pd.read_pickle('/kaggle/input/dataset-with-validation/valid_set.pkl')\n    ","metadata":{"execution":{"iopub.status.busy":"2024-05-12T21:24:08.700554Z","iopub.execute_input":"2024-05-12T21:24:08.700824Z","iopub.status.idle":"2024-05-12T21:24:08.711088Z","shell.execute_reply.started":"2024-05-12T21:24:08.700794Z","shell.execute_reply":"2024-05-12T21:24:08.710171Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def create_small_dataset(train):\n    small_dataset = train.sample(n=10000)\n    small_dataset.to_pickle('small_train.pkl')\n# create_small_dataset(train_set)","metadata":{"execution":{"iopub.status.busy":"2024-05-12T21:24:08.821864Z","iopub.execute_input":"2024-05-12T21:24:08.822183Z","iopub.status.idle":"2024-05-12T21:24:08.827058Z","shell.execute_reply.started":"2024-05-12T21:24:08.822157Z","shell.execute_reply":"2024-05-12T21:24:08.826085Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# remove this, we do it our way, which I guess is better\nLOG_FEATURES = ['X4_mean', 'X11_mean', 'X18_mean', 'X50_mean', 'X26_mean', 'X3112_mean']\n\ny_train = np.zeros_like(train[CONFIG.TARGET_COLUMNS], dtype=np.float32)\nfor target_idx, target in enumerate(CONFIG.TARGET_COLUMNS):\n    v = train[target].values\n    if target in LOG_FEATURES:\n        v = np.log10(v)\n    y_train[:, target_idx] = v\n\nSCALER = StandardScaler()\ny_train = SCALER.fit_transform(y_train)","metadata":{"execution":{"iopub.status.busy":"2024-05-12T21:24:09.269289Z","iopub.execute_input":"2024-05-12T21:24:09.269660Z","iopub.status.idle":"2024-05-12T21:24:09.290576Z","shell.execute_reply.started":"2024-05-12T21:24:09.269630Z","shell.execute_reply":"2024-05-12T21:24:09.289894Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MEAN = np.array([0.485, 0.456, 0.406])\nSTD = np.array([0.229, 0.224, 0.225])\n\nTRAIN_TRANSFORMS = A.Compose([\n        A.HorizontalFlip(p=0.5),\n        A.RandomSizedCrop(\n            [448, 512],\n            CONFIG.IMAGE_SIZE, CONFIG.IMAGE_SIZE, w2h_ratio=1.0, p=0.75),\n        A.Resize(CONFIG.IMAGE_SIZE, CONFIG.IMAGE_SIZE),\n        A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.25),\n        A.ImageCompression(quality_lower=85, quality_upper=100, p=0.25),\n        A.ToFloat(),\n        A.Normalize(mean=MEAN, std=STD, max_pixel_value=1),\n        ToTensorV2(),\n    ])\n\nTEST_TRANSFORMS = A.Compose([\n        A.Resize(CONFIG.IMAGE_SIZE, CONFIG.IMAGE_SIZE),\n        A.ToFloat(),\n        A.Normalize(mean=MEAN, std=STD, max_pixel_value=1),\n        ToTensorV2(),\n    ])\n\nclass Dataset(Dataset):\n    def __init__(self, X_jpeg_bytes, X_tabular, y, transforms=None):\n        self.X_jpeg_bytes = X_jpeg_bytes\n        self.X_tabular = X_tabular\n        self.y = y\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.X_jpeg_bytes)\n\n    def __getitem__(self, index):\n        image = self.transforms(\n            image=imageio.imread(self.X_jpeg_bytes[index]),\n        )['image']\n        \n        tabular_data = self.X_tabular[index]\n\n        y_sample = self.y[index]\n  \n        return image, tabular_data, y_sample\n\n    \nif CONFIG.USE_VALID_SET is True:\n    y_valid = np.zeros_like(valid_set[CONFIG.TARGET_COLUMNS], dtype=np.float32)\n    for target_idx, target in enumerate(CONFIG.TARGET_COLUMNS):\n        v = valid_set[target].values\n        if target in LOG_FEATURES:\n            v = np.log10(v)\n        y_valid[:, target_idx] = v\n\n    SCALER = StandardScaler()\n    y_valid = SCALER.fit_transform(y_valid)\n    valid_dataset = Dataset( \n        valid_set['jpeg_bytes'].values,\n        y_valid,\n        TEST_TRANSFORMS,\n    )\n    valid_dataloader = DataLoader(\n        valid_dataset,\n        shuffle=False,\n        batch_size=CONFIG.BATCH_SIZE,\n        num_workers=psutil.cpu_count(),\n)\nmean_columns = ['X4_mean', 'X11_mean', 'X18_mean', 'X50_mean', 'X26_mean', 'X3112_mean']\nsd_columns = [col for col in train.columns if col.endswith('_sd')]\ntrain_tabular = train.drop(columns=sd_columns+mean_columns+['id','jpeg_bytes'])\ntrain_dataset = Dataset(\n    train['jpeg_bytes'].values,\n    train_tabular.values,\n    y_train,\n    TRAIN_TRANSFORMS,\n)\n\ntrain_dataloader = DataLoader(\n        train_dataset,\n        batch_size=CONFIG.BATCH_SIZE,\n        shuffle=True,\n        drop_last=True,\n        num_workers=psutil.cpu_count(),\n)\n\ntest_tabular = train.drop(columns=sd_columns+mean_columns+['id','jpeg_bytes'])\ntest_dataset = Dataset(\n    test['jpeg_bytes'].values,\n    test_tabular.values,\n    test['id'].values,\n    TEST_TRANSFORMS,\n)   ","metadata":{"execution":{"iopub.status.busy":"2024-05-12T21:28:17.264205Z","iopub.execute_input":"2024-05-12T21:28:17.265121Z","iopub.status.idle":"2024-05-12T21:28:18.617521Z","shell.execute_reply.started":"2024-05-12T21:28:17.265089Z","shell.execute_reply":"2024-05-12T21:28:18.616489Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"class MLPModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.backbone = timm.create_model(\n            CONFIG.BACKBONE,\n            pretrained=True,\n            num_classes=0,  # Disable final classification layer\n            features_only=True, \n            out_indices=[-1] \n        )\n        backbone_out_features = self.backbone.feature_info.channels()[-1]\n\n        self.image_mlp = nn.Sequential(\n            nn.Linear(backbone_out_features, 512),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(512, CONFIG.IMAGES_OUTPUT_DIM),\n            nn.ReLU(),\n            nn.Dropout(0.5)\n        )\n\n        self.tabular_mlp = nn.Sequential(\n            nn.Linear(CONFIG.TABULAR_INPUT_DIM, 512),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(512, CONFIG.TABULAR_OUTPUT_DIM),\n            nn.ReLU(),\n            nn.Dropout(0.5)\n        )\n\n        self.combined_mlp = nn.Sequential(\n            nn.Linear(CONFIG.IMAGES_OUTPUT_DIM + CONFIG.TABULAR_OUTPUT_DIM, 256),  # Combine features from image and tabular data\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(256, CONFIG.N_TARGETS)\n        )\n        \n    def forward(self, images, tabular_data):\n        image_features = self.backbone(images)[-1]\n        image_features = torch.flatten(image_features, start_dim=1)  # Flatten the features\n        processed_image_features = self.image_mlp(image_features)\n        \n        tabular_features = self.tabular_mlp(tabular_data)\n        \n        combined_features = torch.cat([processed_image_features, tabular_features], dim=1)\n        \n        output = self.combined_mlp(combined_features)\n        return output","metadata":{"execution":{"iopub.status.busy":"2024-05-12T21:28:19.319509Z","iopub.execute_input":"2024-05-12T21:28:19.319877Z","iopub.status.idle":"2024-05-12T21:28:19.331929Z","shell.execute_reply.started":"2024-05-12T21:28:19.319850Z","shell.execute_reply":"2024-05-12T21:28:19.330973Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.backbone = timm.create_model(\n                CONFIG.BACKBONE,\n                num_classes=CONFIG.N_TARGETS,\n                pretrained=True)\n        \n    def forward(self, inputs):\n        return self.backbone(inputs)\n\nmodel = MLPModel()#Model()\nmodel = model.to('cuda')\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2024-05-12T21:28:19.849120Z","iopub.execute_input":"2024-05-12T21:28:19.849462Z","iopub.status.idle":"2024-05-12T21:28:24.302366Z","shell.execute_reply.started":"2024-05-12T21:28:19.849434Z","shell.execute_reply":"2024-05-12T21:28:24.301437Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"MLPModel(\n  (backbone): FeatureListNet(\n    (patch_embed): PatchEmbed(\n      (proj): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))\n      (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n    )\n    (layers_0): SwinTransformerStage(\n      (downsample): Identity()\n      (blocks): Sequential(\n        (0): SwinTransformerBlock(\n          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n          (attn): WindowAttention(\n            (qkv): Linear(in_features=192, out_features=576, bias=True)\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=192, out_features=192, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n            (softmax): Softmax(dim=-1)\n          )\n          (drop_path1): Identity()\n          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=192, out_features=768, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=768, out_features=192, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (drop_path2): Identity()\n        )\n        (1): SwinTransformerBlock(\n          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n          (attn): WindowAttention(\n            (qkv): Linear(in_features=192, out_features=576, bias=True)\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=192, out_features=192, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n            (softmax): Softmax(dim=-1)\n          )\n          (drop_path1): DropPath(drop_prob=0.004)\n          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=192, out_features=768, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=768, out_features=192, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (drop_path2): DropPath(drop_prob=0.004)\n        )\n      )\n    )\n    (layers_1): SwinTransformerStage(\n      (downsample): PatchMerging(\n        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (reduction): Linear(in_features=768, out_features=384, bias=False)\n      )\n      (blocks): Sequential(\n        (0): SwinTransformerBlock(\n          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n          (attn): WindowAttention(\n            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=384, out_features=384, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n            (softmax): Softmax(dim=-1)\n          )\n          (drop_path1): DropPath(drop_prob=0.009)\n          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (drop_path2): DropPath(drop_prob=0.009)\n        )\n        (1): SwinTransformerBlock(\n          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n          (attn): WindowAttention(\n            (qkv): Linear(in_features=384, out_features=1152, bias=True)\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=384, out_features=384, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n            (softmax): Softmax(dim=-1)\n          )\n          (drop_path1): DropPath(drop_prob=0.013)\n          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (drop_path2): DropPath(drop_prob=0.013)\n        )\n      )\n    )\n    (layers_2): SwinTransformerStage(\n      (downsample): PatchMerging(\n        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n        (reduction): Linear(in_features=1536, out_features=768, bias=False)\n      )\n      (blocks): Sequential(\n        (0): SwinTransformerBlock(\n          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (attn): WindowAttention(\n            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=768, out_features=768, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n            (softmax): Softmax(dim=-1)\n          )\n          (drop_path1): DropPath(drop_prob=0.017)\n          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (drop_path2): DropPath(drop_prob=0.017)\n        )\n        (1): SwinTransformerBlock(\n          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (attn): WindowAttention(\n            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=768, out_features=768, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n            (softmax): Softmax(dim=-1)\n          )\n          (drop_path1): DropPath(drop_prob=0.022)\n          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (drop_path2): DropPath(drop_prob=0.022)\n        )\n        (2): SwinTransformerBlock(\n          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (attn): WindowAttention(\n            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=768, out_features=768, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n            (softmax): Softmax(dim=-1)\n          )\n          (drop_path1): DropPath(drop_prob=0.026)\n          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (drop_path2): DropPath(drop_prob=0.026)\n        )\n        (3): SwinTransformerBlock(\n          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (attn): WindowAttention(\n            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=768, out_features=768, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n            (softmax): Softmax(dim=-1)\n          )\n          (drop_path1): DropPath(drop_prob=0.030)\n          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (drop_path2): DropPath(drop_prob=0.030)\n        )\n        (4): SwinTransformerBlock(\n          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (attn): WindowAttention(\n            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=768, out_features=768, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n            (softmax): Softmax(dim=-1)\n          )\n          (drop_path1): DropPath(drop_prob=0.035)\n          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (drop_path2): DropPath(drop_prob=0.035)\n        )\n        (5): SwinTransformerBlock(\n          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (attn): WindowAttention(\n            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=768, out_features=768, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n            (softmax): Softmax(dim=-1)\n          )\n          (drop_path1): DropPath(drop_prob=0.039)\n          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (drop_path2): DropPath(drop_prob=0.039)\n        )\n        (6): SwinTransformerBlock(\n          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (attn): WindowAttention(\n            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=768, out_features=768, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n            (softmax): Softmax(dim=-1)\n          )\n          (drop_path1): DropPath(drop_prob=0.043)\n          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (drop_path2): DropPath(drop_prob=0.043)\n        )\n        (7): SwinTransformerBlock(\n          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (attn): WindowAttention(\n            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=768, out_features=768, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n            (softmax): Softmax(dim=-1)\n          )\n          (drop_path1): DropPath(drop_prob=0.048)\n          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (drop_path2): DropPath(drop_prob=0.048)\n        )\n        (8): SwinTransformerBlock(\n          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (attn): WindowAttention(\n            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=768, out_features=768, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n            (softmax): Softmax(dim=-1)\n          )\n          (drop_path1): DropPath(drop_prob=0.052)\n          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (drop_path2): DropPath(drop_prob=0.052)\n        )\n        (9): SwinTransformerBlock(\n          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (attn): WindowAttention(\n            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=768, out_features=768, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n            (softmax): Softmax(dim=-1)\n          )\n          (drop_path1): DropPath(drop_prob=0.057)\n          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (drop_path2): DropPath(drop_prob=0.057)\n        )\n        (10): SwinTransformerBlock(\n          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (attn): WindowAttention(\n            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=768, out_features=768, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n            (softmax): Softmax(dim=-1)\n          )\n          (drop_path1): DropPath(drop_prob=0.061)\n          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (drop_path2): DropPath(drop_prob=0.061)\n        )\n        (11): SwinTransformerBlock(\n          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (attn): WindowAttention(\n            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=768, out_features=768, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n            (softmax): Softmax(dim=-1)\n          )\n          (drop_path1): DropPath(drop_prob=0.065)\n          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (drop_path2): DropPath(drop_prob=0.065)\n        )\n        (12): SwinTransformerBlock(\n          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (attn): WindowAttention(\n            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=768, out_features=768, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n            (softmax): Softmax(dim=-1)\n          )\n          (drop_path1): DropPath(drop_prob=0.070)\n          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (drop_path2): DropPath(drop_prob=0.070)\n        )\n        (13): SwinTransformerBlock(\n          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (attn): WindowAttention(\n            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=768, out_features=768, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n            (softmax): Softmax(dim=-1)\n          )\n          (drop_path1): DropPath(drop_prob=0.074)\n          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (drop_path2): DropPath(drop_prob=0.074)\n        )\n        (14): SwinTransformerBlock(\n          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (attn): WindowAttention(\n            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=768, out_features=768, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n            (softmax): Softmax(dim=-1)\n          )\n          (drop_path1): DropPath(drop_prob=0.078)\n          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (drop_path2): DropPath(drop_prob=0.078)\n        )\n        (15): SwinTransformerBlock(\n          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (attn): WindowAttention(\n            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=768, out_features=768, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n            (softmax): Softmax(dim=-1)\n          )\n          (drop_path1): DropPath(drop_prob=0.083)\n          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (drop_path2): DropPath(drop_prob=0.083)\n        )\n        (16): SwinTransformerBlock(\n          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (attn): WindowAttention(\n            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=768, out_features=768, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n            (softmax): Softmax(dim=-1)\n          )\n          (drop_path1): DropPath(drop_prob=0.087)\n          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (drop_path2): DropPath(drop_prob=0.087)\n        )\n        (17): SwinTransformerBlock(\n          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (attn): WindowAttention(\n            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=768, out_features=768, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n            (softmax): Softmax(dim=-1)\n          )\n          (drop_path1): DropPath(drop_prob=0.091)\n          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (drop_path2): DropPath(drop_prob=0.091)\n        )\n      )\n    )\n    (layers_3): SwinTransformerStage(\n      (downsample): PatchMerging(\n        (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n        (reduction): Linear(in_features=3072, out_features=1536, bias=False)\n      )\n      (blocks): Sequential(\n        (0): SwinTransformerBlock(\n          (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n          (attn): WindowAttention(\n            (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=1536, out_features=1536, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n            (softmax): Softmax(dim=-1)\n          )\n          (drop_path1): DropPath(drop_prob=0.096)\n          (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=6144, out_features=1536, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (drop_path2): DropPath(drop_prob=0.096)\n        )\n        (1): SwinTransformerBlock(\n          (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n          (attn): WindowAttention(\n            (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n            (attn_drop): Dropout(p=0.0, inplace=False)\n            (proj): Linear(in_features=1536, out_features=1536, bias=True)\n            (proj_drop): Dropout(p=0.0, inplace=False)\n            (softmax): Softmax(dim=-1)\n          )\n          (drop_path1): DropPath(drop_prob=0.100)\n          (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n          (mlp): Mlp(\n            (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n            (act): GELU(approximate='none')\n            (drop1): Dropout(p=0.0, inplace=False)\n            (norm): Identity()\n            (fc2): Linear(in_features=6144, out_features=1536, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (drop_path2): DropPath(drop_prob=0.100)\n        )\n      )\n    )\n  )\n  (image_mlp): Sequential(\n    (0): Linear(in_features=1536, out_features=512, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.5, inplace=False)\n    (3): Linear(in_features=512, out_features=128, bias=True)\n    (4): ReLU()\n    (5): Dropout(p=0.5, inplace=False)\n  )\n  (tabular_mlp): Sequential(\n    (0): Linear(in_features=128, out_features=512, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.5, inplace=False)\n    (3): Linear(in_features=512, out_features=128, bias=True)\n    (4): ReLU()\n    (5): Dropout(p=0.5, inplace=False)\n  )\n  (combined_mlp): Sequential(\n    (0): Linear(in_features=256, out_features=256, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.5, inplace=False)\n    (3): Linear(in_features=256, out_features=6, bias=True)\n  )\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"def get_lr_scheduler(optimizer):\n    return torch.optim.lr_scheduler.OneCycleLR(\n        optimizer=optimizer,\n        max_lr=CONFIG.LR_MAX,\n        total_steps=CONFIG.N_STEPS,\n        pct_start=0.1,\n        anneal_strategy='cos',\n        div_factor=1e1,\n        final_div_factor=1e1,\n    )\n#     return torch.optim.lr_scheduler.ReduceLROnPlateau(\n#         optimizer, \n#         mode='min', \n#         factor=0.1, \n#         patience=10, \n#         verbose=True)\n\n\n\nclass AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val):\n        self.sum += val.sum()\n        self.count += val.numel()\n        self.avg = self.sum / self.count\n\nMAE = torchmetrics.regression.MeanAbsoluteError().to('cuda')\nR2 = torchmetrics.regression.R2Score(num_outputs=CONFIG.N_TARGETS, multioutput='uniform_average').to('cuda')\nLOSS = AverageMeter()\n\nY_MEAN = torch.tensor(y_train).mean(dim=0).to('cuda')\nEPS = torch.tensor([1e-6]).to('cuda')\n\ndef r2_loss(y_pred, y_true):\n    ss_res = torch.sum((y_true - y_pred)**2, dim=0)\n    ss_total = torch.sum((y_true - Y_MEAN)**2, dim=0)\n    ss_total = torch.maximum(ss_total, EPS)\n    r2 = torch.mean(ss_res / ss_total)\n    return r2\n\nLOSS_FN = nn.SmoothL1Loss() # r2_loss\n\noptimizer = torch.optim.AdamW(\n    params=model.parameters(),\n    lr=CONFIG.LR_MAX,\n    weight_decay=CONFIG.WEIGHT_DECAY,\n)\n\nLR_SCHEDULER = get_lr_scheduler(optimizer)","metadata":{"execution":{"iopub.status.busy":"2024-05-12T21:28:24.303948Z","iopub.execute_input":"2024-05-12T21:28:24.304244Z","iopub.status.idle":"2024-05-12T21:28:24.326098Z","shell.execute_reply.started":"2024-05-12T21:28:24.304218Z","shell.execute_reply":"2024-05-12T21:28:24.325329Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"if CONFIG.TRAIN_MODEL is True:\n    print(\"Start Training:\")\n    for epoch in range(CONFIG.N_EPOCHS):\n        MAE.reset()\n        R2.reset()\n        LOSS.reset()\n        model.train()\n\n        for step, (X_images, X_tabular, y_true)  in enumerate(train_dataloader):\n            X_images = X_images.to('cuda')\n            X_tabular = X_tabular.to('cuda')\n            y_true = y_true.to('cuda')\n            t_start = time.perf_counter_ns()\n            y_pred = model(X_images, X_tabular)\n            loss = LOSS_FN(y_pred, y_true)\n            LOSS.update(loss)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n            LR_SCHEDULER.step()\n            MAE.update(y_pred, y_true)\n            R2.update(y_pred, y_true)\n\n            if not CONFIG.IS_INTERACTIVE and (step+1) == CONFIG.N_STEPS_PER_EPOCH:\n                print(\n                    f'EPOCH {epoch+1:02d}, {step+1:04d}/{CONFIG.N_STEPS_PER_EPOCH} | ' + \n                    f'loss: {LOSS.avg:.4f}, mae: {MAE.compute().item():.4f}, r2: {R2.compute().item():.4f}, ' +\n                    f'step: {(time.perf_counter_ns()-t_start)*1e-9:.3f}s, lr: {LR_SCHEDULER.get_last_lr()[0]:.2e}',\n                )\n            elif CONFIG.IS_INTERACTIVE:\n                print(\n                    f'\\rEPOCH {epoch+1:02d}, {step+1:04d}/{CONFIG.N_STEPS_PER_EPOCH} | ' + \n                    f'loss: {LOSS.avg:.4f}, mae: {MAE.compute().item():.4f}, r2: {R2.compute().item():.4f}, ' +\n                    f'step: {(time.perf_counter_ns()-t_start)*1e-9:.3f}s, lr: {LR_SCHEDULER.get_last_lr()[0]:.2e}',\n                    end='\\n' if (step + 1) == CONFIG.N_STEPS_PER_EPOCH else '', flush=True,\n                )\n\n    torch.save(model, 'model.pth')\nelse:\n    model = torch.load('/kaggle/input/baseline-model/model.pth')","metadata":{"execution":{"iopub.status.busy":"2024-05-12T21:28:24.328861Z","iopub.execute_input":"2024-05-12T21:28:24.329098Z","iopub.status.idle":"2024-05-12T21:28:25.222910Z","shell.execute_reply.started":"2024-05-12T21:28:24.329077Z","shell.execute_reply":"2024-05-12T21:28:25.220184Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"Start Training:\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[38], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m LOSS\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, (X_images, X_tabular, y_true)  \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m     10\u001b[0m     X_images \u001b[38;5;241m=\u001b[39m X_images\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m     X_tabular \u001b[38;5;241m=\u001b[39m X_tabular\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1345\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1344\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1371\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1371\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_utils.py:694\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    691\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    693\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 694\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n","\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 265, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 142, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 142, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 119, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 169, in collate_numpy_array_fn\n    raise TypeError(default_collate_err_msg_format.format(elem.dtype))\nTypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found object\n"],"ename":"TypeError","evalue":"Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 265, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 142, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 142, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 119, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 169, in collate_numpy_array_fn\n    raise TypeError(default_collate_err_msg_format.format(elem.dtype))\nTypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found object\n","output_type":"error"}]},{"cell_type":"markdown","source":"### Evaluation of the model","metadata":{}},{"cell_type":"code","source":"if CONFIG.USE_VALID_SET is True:\n    model.eval()  # Set the model to evaluation mode\n    test_MAE = torchmetrics.regression.MeanAbsoluteError().to('cuda')\n    test_R2 = torchmetrics.regression.R2Score(num_outputs=CONFIG.N_TARGETS, multioutput='uniform_average').to('cuda')\n    test_loss = AverageMeter()\n\n    with torch.no_grad():  # Disable gradient computation for evaluation\n        for X_batch, y_true in valid_dataloader:\n            X_batch = X_batch.to('cuda')\n            y_true = y_true.to('cuda')\n\n            # Predict and evaluate\n            y_pred = model(X_batch)\n            loss = LOSS_FN(y_pred, y_true)\n            test_loss.update(loss)\n            test_MAE.update(y_pred, y_true)\n            test_R2.update(y_pred, y_true)\n\n    # Print test results\n    print(\n        f'Test Results - Loss: {test_loss.avg:.4f}, MAE: {test_MAE.compute().item():.4f}, ' +\n        f'R2: {test_R2.compute().item():.4f}'\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SUBMISSION_ROWS = []\nmodel.eval()\n\nfor X_images_test,X_tabular_test, test_id in tqdm(test_dataset):\n    with torch.no_grad():\n        y_pred = model(X_images_test.unsqueeze(0).to('cuda'), X_tabular_test.unsqueeze(0).to('cuda')).detach().cpu().numpy()\n    \n    y_pred = SCALER.inverse_transform(y_pred).squeeze()\n    row = {'id': test_id}\n    \n    for k, v in zip(CONFIG.TARGET_COLUMNS, y_pred):\n        if k in LOG_FEATURES:\n            row[k.replace('_mean', '')] = 10 ** v\n        else:\n            row[k.replace('_mean', '')] = v\n\n    SUBMISSION_ROWS.append(row)\n    \nsubmission_df = pd.DataFrame(SUBMISSION_ROWS)\nsubmission_df.to_csv('submission.csv', index=False)\nprint(\"Submit!\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Add the XGBoost model predictions","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport xgboost as xgb\nfrom tensorflow.keras.applications.resnet50 import preprocess_input\n# from tensorflow.keras.applications.efficientnet import preprocess_input\nfrom tensorflow.keras.applications import EfficientNetB0\n\nRESNET = False\nEFFICIENT = True\n\nif RESNET:\n    image_xy = [180,180]\nelif EFFICIENT:\n    image_xy = [300,300]\n    \nif CONFIG.USE_XGBOOST is True:\n#     train_features_xgb = pd.read_pickle('/kaggle/input/features-xgb/extracted_features.pkl')\n    train_features_xgb = pd.read_pickle('/kaggle/input/efficientnet/train_eff.pkl')\n    X_full = train_features_xgb.drop(columns=CONFIG.TARGET_COLUMNS+['id'])\n    Y_full = train_features_xgb[CONFIG.TARGET_COLUMNS]\n    models = {}\n    for column in Y_full.columns:\n        model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=250, learning_rate=0.038, max_depth=10)\n        model.load_model(f\"/kaggle/input/xgboost-finetuned-0-2393/model{column}.json\")\n        models[column] = model\n        image_model = EfficientNetB0(weights='imagenet', include_top=False, pooling='avg')\n\n    # Define the function to create a TensorFlow dataset for images\n    def create_dataset(image_paths, batch_size=128):\n        def process_path(file_path):\n            img = tf.io.read_file(file_path)\n            img = tf.image.decode_jpeg(img, channels=3)\n            img = tf.image.resize(img, image_xy)\n            if RESNET:\n                img = preprocess_input(img)\n            return img\n        path_ds = tf.data.Dataset.from_tensor_slices(image_paths)\n        image_ds = path_ds.map(process_path, num_parallel_calls=tf.data.AUTOTUNE)\n        image_ds = image_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n        return image_ds\n\n    def extract_features_with_dataset(dataset, df):\n        features_list = []\n        for batch_imgs in dataset:\n            print(\".\", end=\"\")  # Print progress\n            features = image_model.predict(batch_imgs, verbose=0)\n            features_list.extend(features)\n        features_array = np.array(features_list)\n\n        # Convert the features array into a DataFrame\n        features_df = pd.DataFrame(features_array)\n\n        features_df.columns = [f'feature_{i}' for i in range(features_array.shape[1])]\n\n        new_df = pd.concat([df.reset_index(drop=True), features_df.reset_index(drop=True)], axis=1)\n\n        return new_df\n\n    test_image_folder = '/kaggle/input/planttraits2024/test_images'\n    test = pd.read_csv('/kaggle/input/planttraits2024/test.csv')\n    image_paths = [os.path.join(test_image_folder, f\"{img_id}.jpeg\") for img_id in test['id']]\n\n    # Create the dataset\n    image_dataset = create_dataset(image_paths)\n\n    test_copy = test.drop(columns = 'id')\n    \n    # Extract features and directly insert them into the DataFrame as separate columns\n    test_XGB = extract_features_with_dataset(image_dataset, test_copy)\n\n    print(test_XGB.head())\n    mean_values = Y_full.mean()\n    submission = pd.DataFrame({'id': test['id']})\n    submission[Y_full.columns] = CONFIG.TARGET_COLUMNS\n\n    #rename from _mean\n    submission.columns = submission.columns.str.replace('_mean', '')\n    submission['X4'] = models['X4_mean'].predict(test_XGB)\n    submission['X11'] = models['X11_mean'].predict(test_XGB)\n    submission['X18'] = models['X18_mean'].predict(test_XGB)\n    submission['X50'] = models['X50_mean'].predict(test_XGB)\n    submission['X26'] = models['X26_mean'].predict(test_XGB)\n    submission['X3112'] = models['X3112_mean'].predict(test_XGB)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if CONFIG.USE_XGBOOST is True:\n    combined_submission = pd.DataFrame()\n    combined_submission['id'] = submission['id']\n    for column in Y_full.columns.str.replace('_mean', ''):\n        combined_submission[column] = submission[column]*0.3 + submission_df[column]*0.7\n\n    combined_submission.to_csv('submission.csv', index=False)\n    print(\"submission ready!\")\n    combined_submission","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}