{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":65626,"databundleVersionId":8046133,"sourceType":"competition"},{"sourceId":8145845,"sourceType":"datasetVersion","datasetId":4816952},{"sourceId":8149784,"sourceType":"datasetVersion","datasetId":4819871},{"sourceId":8178456,"sourceType":"datasetVersion","datasetId":4841404},{"sourceId":8204134,"sourceType":"datasetVersion","datasetId":4860734},{"sourceId":8332370,"sourceType":"datasetVersion","datasetId":4862895}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"- Notebook modified from https://www.kaggle.com/code/markwijkhuizen/planttraits2024-eda-training-pub.\n- Training only, EDA part not included.","metadata":{}},{"cell_type":"markdown","source":"# Setup","metadata":{"execution":{"iopub.status.busy":"2024-05-15T17:00:54.879370Z","iopub.execute_input":"2024-05-15T17:00:54.880227Z","iopub.status.idle":"2024-05-15T17:00:54.883948Z","shell.execute_reply.started":"2024-05-15T17:00:54.880196Z","shell.execute_reply":"2024-05-15T17:00:54.883047Z"}}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport imageio.v3 as imageio\nimport albumentations as A\n\nfrom albumentations.pytorch import ToTensorV2\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch import nn\nfrom tqdm.notebook import tqdm\nfrom sklearn.preprocessing import StandardScaler\n\nimport torch\nimport timm\nimport glob\nimport torchmetrics\nimport time\nimport psutil\nimport os\n\ntqdm.pandas()","metadata":{"execution":{"iopub.status.busy":"2024-05-15T18:53:40.509150Z","iopub.execute_input":"2024-05-15T18:53:40.510021Z","iopub.status.idle":"2024-05-15T18:53:57.276193Z","shell.execute_reply.started":"2024-05-15T18:53:40.509985Z","shell.execute_reply":"2024-05-15T18:53:57.275353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Config():\n    IMAGE_SIZE = 384 #196#224\n    BACKBONE = 'swin_large_patch4_window12_384.ms_in22k_ft_in1k'#'eva_large_patch14_196.in22k_ft_in22k_in1k'#'vit_base_patch16_clip_224.openai_ft_in12k'#'efficientnet_b0' #\n    TARGET_COLUMNS = ['X4_mean', 'X11_mean', 'X18_mean', 'X50_mean', 'X26_mean', 'X3112_mean']\n    N_TARGETS = len(TARGET_COLUMNS)\n    BATCH_SIZE = 10\n    LR_MAX = 1e-4\n    WEIGHT_DECAY = 0.01\n    N_EPOCHS = 6\n    TRAIN_MODEL = True\n    IS_INTERACTIVE = os.environ['KAGGLE_KERNEL_RUN_TYPE'] == 'Interactive'\n    BUILD_PKL_DATASET = False\n    BUILD_VALID_SET = False\n    USE_SMALL_DATASET = False\n    USE_VALID_SET = False\n    USE_MODIFIED_TRAIN = False\n    USE_XGBOOST = False\n    USE_PREPROCESSING = False\n    IMAGES_OUTPUT_DIM = 128\n    TABULAR_OUTPUT_DIM = 128\n    # was previously set to 128 but do not know to what this corresponds,\n    # the tabular data are 163\n    TABULAR_INPUT_DIM = 163 # 128\n        \nCONFIG = Config()","metadata":{"execution":{"iopub.status.busy":"2024-05-15T18:53:57.278204Z","iopub.execute_input":"2024-05-15T18:53:57.278665Z","iopub.status.idle":"2024-05-15T18:53:57.285983Z","shell.execute_reply.started":"2024-05-15T18:53:57.278631Z","shell.execute_reply":"2024-05-15T18:53:57.285061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create or load dataset","metadata":{"execution":{"iopub.status.busy":"2024-05-15T17:01:16.555871Z","iopub.execute_input":"2024-05-15T17:01:16.556491Z","iopub.status.idle":"2024-05-15T17:01:16.560424Z","shell.execute_reply.started":"2024-05-15T17:01:16.556459Z","shell.execute_reply":"2024-05-15T17:01:16.559550Z"}}},{"cell_type":"code","source":"if CONFIG.BUILD_PKL_DATASET is True:\n    train = pd.read_csv('/kaggle/input/planttraits2024/train.csv')\n    train['file_path'] = train['id'].apply(lambda s: f'/kaggle/input/planttraits2024/train_images/{s}.jpeg')\n    train['jpeg_bytes'] = train['file_path'].progress_apply(lambda fp: open(fp, 'rb').read())\n    train.to_pickle('train.pkl')\n    test = pd.read_csv('/kaggle/input/planttraits2024/test.csv')\n    test['file_path'] = test['id'].apply(lambda s: f'/kaggle/input/planttraits2024/test_images/{s}.jpeg')\n    test['jpeg_bytes'] = test['file_path'].progress_apply(lambda fp: open(fp, 'rb').read())\n    test.to_pickle('test.pkl')\nelse:\n    if CONFIG.USE_SMALL_DATASET is True:\n        train = pd.read_pickle('/kaggle/input/dataset-with-validation/small_train.pkl')       \n    elif CONFIG.USE_MODIFIED_TRAIN is True:\n        train = pd.read_pickle('/kaggle/input/dataset-with-validation/train_set.pkl')\n    else:\n         train = pd.read_pickle('/kaggle/input/baseline-model/train.pkl')\n    test = pd.read_pickle('/kaggle/input/baseline-model/test.pkl')\n    \nfor column in CONFIG.TARGET_COLUMNS:\n    lower_quantile = train[column].quantile(0.005)\n    upper_quantile = train[column].quantile(0.985)  \n    train = train[(train[column] >= lower_quantile) & (train[column] <= upper_quantile)]\n\nCONFIG.N_TRAIN_SAMPLES = len(train)\nCONFIG.N_STEPS_PER_EPOCH = (CONFIG.N_TRAIN_SAMPLES // CONFIG.BATCH_SIZE)\nCONFIG.N_STEPS = CONFIG.N_STEPS_PER_EPOCH * CONFIG.N_EPOCHS + 1\n\nprint('N_TRAIN_SAMPLES:', len(train), 'N_TEST_SAMPLES:', len(test))","metadata":{"execution":{"iopub.status.busy":"2024-05-15T18:53:57.287156Z","iopub.execute_input":"2024-05-15T18:53:57.287440Z","iopub.status.idle":"2024-05-15T18:54:30.301314Z","shell.execute_reply.started":"2024-05-15T18:53:57.287416Z","shell.execute_reply":"2024-05-15T18:54:30.300362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#all columns must be identical to be consider the same species\ntrait_columns = ['X4_mean', 'X11_mean', 'X18_mean', 'X50_mean', 'X26_mean', 'X3112_mean']\ntrain['species'] = train.groupby(CONFIG.TARGET_COLUMNS).ngroup()\nspecies_counts = train['species'].nunique()\nprint (f\"{species_counts} unique species found in {len(train)} records\")","metadata":{"execution":{"iopub.status.busy":"2024-05-15T18:54:30.303868Z","iopub.execute_input":"2024-05-15T18:54:30.304545Z","iopub.status.idle":"2024-05-15T18:54:30.353588Z","shell.execute_reply.started":"2024-05-15T18:54:30.304507Z","shell.execute_reply":"2024-05-15T18:54:30.352741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"species_counts = train['species'].value_counts()\ncount_summary = species_counts.value_counts().sort_index()\ncount_summary.plot(kind='bar', figsize=(7, 3))\n\nplt.title('Images per Species')\nplt.xlabel('Images')\nplt.ylabel('Species')\nplt.tight_layout() \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-15T18:54:30.354778Z","iopub.execute_input":"2024-05-15T18:54:30.355423Z","iopub.status.idle":"2024-05-15T18:54:30.822419Z","shell.execute_reply.started":"2024-05-15T18:54:30.355391Z","shell.execute_reply":"2024-05-15T18:54:30.821414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train = train[:100]","metadata":{"execution":{"iopub.status.busy":"2024-05-15T18:54:30.823932Z","iopub.execute_input":"2024-05-15T18:54:30.824336Z","iopub.status.idle":"2024-05-15T18:54:30.829395Z","shell.execute_reply.started":"2024-05-15T18:54:30.824298Z","shell.execute_reply":"2024-05-15T18:54:30.828382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing","metadata":{"execution":{"iopub.status.busy":"2024-04-29T15:47:23.898819Z","iopub.execute_input":"2024-04-29T15:47:23.900126Z","iopub.status.idle":"2024-04-29T15:47:23.904603Z","shell.execute_reply.started":"2024-04-29T15:47:23.900083Z","shell.execute_reply":"2024-04-29T15:47:23.903274Z"}}},{"cell_type":"markdown","source":"In the nature article, the authors performed data notmalisation in all the columns. In their case they had less auxilary data. Here, we will preprocess all the data.\n\nDetails for all these can be foud at https://www.kaggle.com/code/vaggelisspi/planttraits-working-through-the-nature-article#Pre-processing.","metadata":{}},{"cell_type":"code","source":"PREPROCESS_COLS = test.columns[1:-2].tolist()\nprint(len(PREPROCESS_COLS))","metadata":{"execution":{"iopub.status.busy":"2024-05-15T18:48:31.820826Z","iopub.execute_input":"2024-05-15T18:48:31.821182Z","iopub.status.idle":"2024-05-15T18:48:31.829331Z","shell.execute_reply.started":"2024-05-15T18:48:31.821149Z","shell.execute_reply":"2024-05-15T18:48:31.828422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if CONFIG.USE_PREPROCESSING is True:\n    # not worring about '_sd' columns for now\n    sd_columns = [col for col in train.columns if col.endswith('_sd')]\n    train = train.drop(columns=sd_columns)\n    # the columns of the auxilary data\n    PREPROCESS_COLS = test.columns[1:-2].tolist()\n    # preprocess the data\n    norm_data = np.zeros_like(train[PREPROCESS_COLS], dtype=np.float32)\n    for col_idx, col in enumerate(PREPROCESS_COLS):\n        v = train[col].values\n        v = np.log10(v)\n        norm_data[:, col_idx] = v\n    # \n    norm_data[norm_data == np.inf] = 0\n    norm_data[norm_data == -np.inf] = 0\n    scaler = StandardScaler()\n    norm_data = scaler.fit_transform(norm_data)\n\n    for col_idx, col in enumerate(PREPROCESS_COLS):\n        v = norm_data[:, col_idx]\n        train[col] = v\n\n    train = train.fillna(0)\n    print(train.head())\n    print(train.isnull().sum().sum())\n    \n    # preprocess the test data\n    norm_data = np.zeros_like(test[PREPROCESS_COLS], dtype=np.float32)\n    for col_idx, col in enumerate(PREPROCESS_COLS):\n        v = test[col].values\n        v = np.log10(v)\n        norm_data[:, col_idx] = v\n\n    norm_data[norm_data == np.inf] = 0\n    norm_data[norm_data == -np.inf] = 0\n    # scaler = StandardScaler()\n    norm_data = scaler.fit_transform(norm_data)\n\n    for col_idx, col in enumerate(PREPROCESS_COLS):\n        v = norm_data[:, col_idx]\n        test[col] = v\n\n    print(test.head())\n    print(test.isnull().sum().sum())","metadata":{"execution":{"iopub.status.busy":"2024-05-15T18:50:23.800890Z","iopub.execute_input":"2024-05-15T18:50:23.801198Z","iopub.status.idle":"2024-05-15T18:50:23.814262Z","shell.execute_reply.started":"2024-05-15T18:50:23.801153Z","shell.execute_reply":"2024-05-15T18:50:23.812876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare and train the model","metadata":{"execution":{"iopub.status.busy":"2024-05-15T17:01:46.274713Z","iopub.execute_input":"2024-05-15T17:01:46.275190Z","iopub.status.idle":"2024-05-15T17:01:46.279344Z","shell.execute_reply.started":"2024-05-15T17:01:46.275151Z","shell.execute_reply":"2024-05-15T17:01:46.278458Z"}}},{"cell_type":"markdown","source":"## Build Validation Set","metadata":{}},{"cell_type":"code","source":"if CONFIG.BUILD_VALID_SET is True:\n    unique_species = train['species'].unique()\n    n_test_species_exclusive = 2\n\n    # Randomly pick exclusive test species\n    test_species_exclusive = np.random.choice(unique_species, n_test_species_exclusive, replace=False)\n\n    # Isolate exclusive test species data\n    exclusive_test_set = train[train['species'].isin(test_species_exclusive)]\n\n    # Remove exclusive test species from main data\n    train_reduced = train[~train['species'].isin(test_species_exclusive)]\n    print(len(train_reduced))\n    test_set_common = pd.DataFrame()\n    for species in train_reduced['species'].unique():\n        species_data = train_reduced[train_reduced['species'] == species]\n        sample = species_data.sample(1, random_state=42) \n        test_set_common = pd.concat([test_set_common, sample])\n\n    # Remove these samples from train\n    train_set = train_reduced.drop(test_set_common.index)\n\n    valid_set = pd.concat([exclusive_test_set, test_set_common])\n#     train_set.to_pickle('train_set.pkl')\n#     valid_set.to_pickle('valid_set.pkl')\n    \n    print(f\"Training set size: {len(train_set)}\")\n    print(f\"Valid set size: {len(final_test_set)}\")\n    print(f\"Unique species in Training set: {train_set['species'].nunique()}\")\n    print(f\"Unique species in Valid set: {final_test_set['species'].nunique()}\")\n    train = train_set\nelif CONFIG.USE_VALID_SET is True:\n    valid_set = pd.read_pickle('/kaggle/input/dataset-with-validation/valid_set.pkl')\n    ","metadata":{"execution":{"iopub.status.busy":"2024-05-15T18:48:31.844199Z","iopub.execute_input":"2024-05-15T18:48:31.844557Z","iopub.status.idle":"2024-05-15T18:48:31.854003Z","shell.execute_reply.started":"2024-05-15T18:48:31.844524Z","shell.execute_reply":"2024-05-15T18:48:31.853272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_small_dataset(train):\n    small_dataset = train.sample(n=10000)\n    small_dataset.to_pickle('small_train.pkl')\n# create_small_dataset(train_set)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T18:48:31.858824Z","iopub.execute_input":"2024-05-15T18:48:31.859255Z","iopub.status.idle":"2024-05-15T18:48:31.865461Z","shell.execute_reply.started":"2024-05-15T18:48:31.859228Z","shell.execute_reply":"2024-05-15T18:48:31.864422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Scale the target features","metadata":{}},{"cell_type":"code","source":"LOG_FEATURES = ['X4_mean', 'X11_mean', 'X18_mean', 'X50_mean', 'X26_mean', 'X3112_mean']\n\ny_train = np.zeros_like(train[CONFIG.TARGET_COLUMNS], dtype=np.float32)\nfor target_idx, target in enumerate(CONFIG.TARGET_COLUMNS):\n    v = train[target].values\n    if target in LOG_FEATURES:\n        v = np.log10(v)\n    y_train[:, target_idx] = v\n\nSCALER = StandardScaler()\ny_train = SCALER.fit_transform(y_train)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T18:54:37.487941Z","iopub.execute_input":"2024-05-15T18:54:37.488867Z","iopub.status.idle":"2024-05-15T18:54:37.500174Z","shell.execute_reply.started":"2024-05-15T18:54:37.488824Z","shell.execute_reply":"2024-05-15T18:54:37.498878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create data loaders and setup the iamge transforms","metadata":{}},{"cell_type":"code","source":"MEAN = np.array([0.485, 0.456, 0.406])\nSTD = np.array([0.229, 0.224, 0.225])\n\nTRAIN_TRANSFORMS = A.Compose([\n        A.HorizontalFlip(p=0.5),\n        A.RandomSizedCrop(\n            [448, 512],\n            CONFIG.IMAGE_SIZE, CONFIG.IMAGE_SIZE, w2h_ratio=1.0, p=0.75),\n        A.Resize(CONFIG.IMAGE_SIZE, CONFIG.IMAGE_SIZE),\n        A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.25),\n        A.ImageCompression(quality_lower=85, quality_upper=100, p=0.25),\n        A.ToFloat(),\n        A.Normalize(mean=MEAN, std=STD, max_pixel_value=1),\n        ToTensorV2(),\n    ])\n\nTEST_TRANSFORMS = A.Compose([\n        A.Resize(CONFIG.IMAGE_SIZE, CONFIG.IMAGE_SIZE),\n        A.ToFloat(),\n        A.Normalize(mean=MEAN, std=STD, max_pixel_value=1),\n        ToTensorV2(),\n    ])\n\n\nclass Dataset(Dataset):\n    \"\"\"\n    Costum Dataset class to load the images and the tabular data\n    \"\"\"\n    def __init__(self, X_jpeg_bytes, X_tabular, y, transforms=None):\n        self.X_jpeg_bytes = X_jpeg_bytes\n        self.X_tabular = X_tabular\n        self.y = y\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.X_jpeg_bytes)\n\n    def __getitem__(self, index):\n        image = self.transforms(\n            image=imageio.imread(self.X_jpeg_bytes[index]),\n        )['image']\n        \n        tabular_data = self.X_tabular[index]\n\n\n        y_sample = self.y[index]\n  \n        return image, tabular_data, y_sample\n#         return tabular_data\n\n    \nif CONFIG.USE_VALID_SET is True:\n    y_valid = np.zeros_like(valid_set[CONFIG.TARGET_COLUMNS], dtype=np.float32)\n    for target_idx, target in enumerate(CONFIG.TARGET_COLUMNS):\n        v = valid_set[target].values\n        if target in LOG_FEATURES:\n            v = np.log10(v)\n        y_valid[:, target_idx] = v\n\n    SCALER = StandardScaler()\n    y_valid = SCALER.fit_transform(y_valid)\n\n    valid_dataset = Dataset( \n        valid_set['jpeg_bytes'].values,\n        y_valid,\n        TEST_TRANSFORMS,\n    )\n\n    valid_dataloader = DataLoader(\n        valid_dataset,\n        shuffle=False,\n        batch_size=CONFIG.BATCH_SIZE,\n        num_workers=psutil.cpu_count(),\n    )\n\nmean_columns = ['X4_mean', 'X11_mean', 'X18_mean', 'X50_mean', 'X26_mean', 'X3112_mean']\nsd_columns = [col for col in train.columns if col.endswith('_sd')]\n\ntrain_tabular = train.drop(columns=sd_columns+mean_columns+['id', 'jpeg_bytes', 'file_path', 'species'])\n\ntrain_dataset = Dataset(\n    train['jpeg_bytes'].values,\n    train_tabular.to_numpy(),\n    y_train,\n    TRAIN_TRANSFORMS,\n)\n\ntrain_dataloader = DataLoader(\n        train_dataset,\n        batch_size=CONFIG.BATCH_SIZE,\n        shuffle=True,\n        drop_last=True,\n        num_workers=psutil.cpu_count(),\n)\n\ntest_tabular = train.drop(columns=sd_columns+mean_columns+['id', 'jpeg_bytes'])\ntest_dataset = Dataset(\n    test['jpeg_bytes'].values,\n    test_tabular.to_numpy(),\n    test['id'],\n    TEST_TRANSFORMS,\n)\n\n\n# for step, (X_images, X_tabular, y_true)  in enumerate(train_dataloader):\n#     print(y_true.double().dtype)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T18:54:39.888800Z","iopub.execute_input":"2024-05-15T18:54:39.889674Z","iopub.status.idle":"2024-05-15T18:54:39.916543Z","shell.execute_reply.started":"2024-05-15T18:54:39.889640Z","shell.execute_reply":"2024-05-15T18:54:39.915328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create and train the model","metadata":{}},{"cell_type":"markdown","source":"### Model with MLP layer after Swin and auxiliary data","metadata":{}},{"cell_type":"code","source":"class MLPModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.backbone = timm.create_model(\n            CONFIG.BACKBONE,\n            pretrained=True,\n            num_classes=0,  # Disable final classification layer\n            features_only=True, \n            out_indices=[-1] \n        )\n        backbone_out_features = self.backbone.feature_info.channels()[-1]\n\n        self.image_mlp = nn.Sequential(\n            nn.Linear(backbone_out_features, 512),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(512, CONFIG.IMAGES_OUTPUT_DIM),\n            nn.ReLU(),\n            nn.Dropout(0.5)\n        )\n\n        self.tabular_mlp = nn.Sequential(\n            nn.Linear(CONFIG.TABULAR_INPUT_DIM, 512),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(512, CONFIG.TABULAR_OUTPUT_DIM),\n            nn.ReLU(),\n            nn.Dropout(0.5)\n        )\n\n        self.combined_mlp = nn.Sequential(\n            nn.Linear(CONFIG.IMAGES_OUTPUT_DIM + CONFIG.TABULAR_OUTPUT_DIM, 256),  # Combine features from image and tabular data\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(256, CONFIG.N_TARGETS)\n        )\n        \n    def forward(self, images, tabular_data):\n        image_features = self.backbone(images)[-1]\n        image_features = torch.flatten(image_features, start_dim=1)  # Flatten the features\n        processed_image_features = self.image_mlp(image_features)\n        \n        tabular_features = self.tabular_mlp(tabular_data)\n        \n        combined_features = torch.cat([processed_image_features, tabular_features], dim=1)\n        \n        output = self.combined_mlp(combined_features)\n        return output","metadata":{"execution":{"iopub.status.busy":"2024-05-15T18:48:31.904675Z","iopub.execute_input":"2024-05-15T18:48:31.905250Z","iopub.status.idle":"2024-05-15T18:48:31.916590Z","shell.execute_reply.started":"2024-05-15T18:48:31.905215Z","shell.execute_reply":"2024-05-15T18:48:31.915622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model to extract the Swin and MLP features","metadata":{}},{"cell_type":"code","source":"class FeatureExtractModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.backbone = timm.create_model(\n            CONFIG.BACKBONE,\n            pretrained=True,\n        )\n\n#         self.tabular_mlp = nn.Sequential(\n#             nn.Linear(CONFIG.TABULAR_INPUT_DIM, 512),\n#             nn.ReLU(),\n#             nn.Dropout(0.5),\n#             nn.Linear(512, CONFIG.TABULAR_OUTPUT_DIM),\n#             nn.ReLU(),\n#             nn.Dropout(0.5)\n#         )\n        \n        self.double()\n    \n    def forward(self, images):\n        image_features = self.backbone(images)[-1]\n        \n#         tabular_features = self.tabular_mlp(tabular_data)[-1]\n        \n#         output = torch.cat([image_features, tabular_features])\n        \n        return image_features","metadata":{"execution":{"iopub.status.busy":"2024-05-15T18:54:47.540317Z","iopub.execute_input":"2024-05-15T18:54:47.540713Z","iopub.status.idle":"2024-05-15T18:54:47.548157Z","shell.execute_reply.started":"2024-05-15T18:54:47.540681Z","shell.execute_reply":"2024-05-15T18:54:47.546962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model = FeatureExtractModel()\n# model = model.to(\"cuda\")\n# # print()\n# x = torch.rand(1, 3, 384, 384).to(\"cuda\")\n# # y = torch.rand(16, CONFIG.TABULAR_INPUT_DIM)\n# # out = model(x, y)\n# out = model(x.double())\n# print(out.shape)\n\n# # for idx, m in enumerate(model.modules()):\n# #     print(idx, '->', m)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T18:48:31.928498Z","iopub.execute_input":"2024-05-15T18:48:31.928836Z","iopub.status.idle":"2024-05-15T18:48:31.936503Z","shell.execute_reply.started":"2024-05-15T18:48:31.928805Z","shell.execute_reply":"2024-05-15T18:48:31.935705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Just a Swin model that predicts the classes","metadata":{}},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.backbone = timm.create_model(\n                CONFIG.BACKBONE,\n                num_classes=CONFIG.N_TARGETS,\n                pretrained=True)\n        \n    def forward(self, inputs):\n        return self.backbone(inputs)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T18:50:26.847270Z","iopub.status.idle":"2024-05-15T18:50:26.847617Z","shell.execute_reply.started":"2024-05-15T18:50:26.847448Z","shell.execute_reply":"2024-05-15T18:50:26.847463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Select the model","metadata":{"execution":{"iopub.status.busy":"2024-05-15T17:54:04.403045Z","iopub.execute_input":"2024-05-15T17:54:04.403980Z","iopub.status.idle":"2024-05-15T17:54:04.408360Z","shell.execute_reply.started":"2024-05-15T17:54:04.403947Z","shell.execute_reply":"2024-05-15T17:54:04.407231Z"}}},{"cell_type":"code","source":"model = FeatureExtractModel() # MLPModel() # Model()\nmodel = model.to('cuda')\n# print(model)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T18:54:50.644790Z","iopub.execute_input":"2024-05-15T18:54:50.645414Z","iopub.status.idle":"2024-05-15T18:55:01.032325Z","shell.execute_reply.started":"2024-05-15T18:54:50.645383Z","shell.execute_reply":"2024-05-15T18:55:01.031438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Extracr features","metadata":{}},{"cell_type":"code","source":"def extract_features():\n    features_list = []\n    for step, (X_images, X_tabular, y_true)  in enumerate(train_dataloader):\n        X_images = X_images.double().to('cuda')\n        y_pred = model(X_images)\n        features_list.extend(y_pred)\n    \n    features_array = np.array(features_list)\n    \n    # Convert the features array into a DataFrame\n    features_df = pd.DataFrame(features_array)\n    \n    features_df.columns = [f'feature_{i}' for i in range(features_array.shape[1])]\n    \n    new_df = pd.concat([df.reset_index(drop=True), features_df.reset_index(drop=True)], axis=1)\n    \n    return new_df\n\ndf = extract_features()\nprint(df.shape)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T18:55:19.436758Z","iopub.execute_input":"2024-05-15T18:55:19.437119Z","iopub.status.idle":"2024-05-15T18:55:29.070305Z","shell.execute_reply.started":"2024-05-15T18:55:19.437092Z","shell.execute_reply":"2024-05-15T18:55:29.068596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.to_pickle(\"swin_features.pkl\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training the model","metadata":{}},{"cell_type":"code","source":"def get_lr_scheduler(optimizer):\n    return torch.optim.lr_scheduler.OneCycleLR(\n        optimizer=optimizer,\n        max_lr=CONFIG.LR_MAX,\n        total_steps=CONFIG.N_STEPS,\n        pct_start=0.1,\n        anneal_strategy='cos',\n        div_factor=1e1,\n        final_div_factor=1e1,\n    )\n#     return torch.optim.lr_scheduler.ReduceLROnPlateau(\n#         optimizer, \n#         mode='min', \n#         factor=0.1, \n#         patience=10, \n#         verbose=True)\n\n\n\nclass AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val):\n        self.sum += val.sum()\n        self.count += val.numel()\n        self.avg = self.sum / self.count\n\nMAE = torchmetrics.regression.MeanAbsoluteError().to('cuda')\nR2 = torchmetrics.regression.R2Score(num_outputs=CONFIG.N_TARGETS, multioutput='uniform_average').to('cuda')\nLOSS = AverageMeter()\n\nY_MEAN = torch.tensor(y_train).mean(dim=0).to('cuda')\nEPS = torch.tensor([1e-6]).to('cuda')\n\ndef r2_loss(y_pred, y_true):\n    ss_res = torch.sum((y_true - y_pred)**2, dim=0)\n    ss_total = torch.sum((y_true - Y_MEAN)**2, dim=0)\n    ss_total = torch.maximum(ss_total, EPS)\n    r2 = torch.mean(ss_res / ss_total)\n    return r2\n\nLOSS_FN = nn.SmoothL1Loss() # r2_loss\n\noptimizer = torch.optim.AdamW(\n    params=model.parameters(),\n    lr=CONFIG.LR_MAX,\n    weight_decay=CONFIG.WEIGHT_DECAY,\n)\n\nLR_SCHEDULER = get_lr_scheduler(optimizer)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T18:48:48.968378Z","iopub.status.idle":"2024-05-15T18:48:48.968930Z","shell.execute_reply.started":"2024-05-15T18:48:48.968654Z","shell.execute_reply":"2024-05-15T18:48:48.968677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# del train_dataloader\n# del X_images\n# del X_tabular\n# del y_true\n# del y_pred","metadata":{"execution":{"iopub.status.busy":"2024-05-15T18:48:48.970697Z","iopub.status.idle":"2024-05-15T18:48:48.971210Z","shell.execute_reply.started":"2024-05-15T18:48:48.970929Z","shell.execute_reply":"2024-05-15T18:48:48.970950Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if CONFIG.TRAIN_MODEL is True:\n    print(\"Start Training:\")\n    for epoch in range(CONFIG.N_EPOCHS):\n        MAE.reset()\n        R2.reset()\n        LOSS.reset()\n        model.train()\n\n        for step, (X_images, X_tabular, y_true)  in enumerate(train_dataloader):\n            print(f\"images shape: {X_images.dtype}, tabular shape: {X_tabular.shape}\")\n            X_images = X_images.double().to('cuda')\n            X_tabular = X_tabular.double().to('cuda')\n            y_true = y_true.double().to('cuda')\n            t_start = time.perf_counter_ns()\n            y_pred = model(X_images, X_tabular)\n            loss = LOSS_FN(y_pred, y_true)\n            LOSS.update(loss)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n            LR_SCHEDULER.step()\n            MAE.update(y_pred, y_true)\n            R2.update(y_pred, y_true)\n\n            if not CONFIG.IS_INTERACTIVE and (step+1) == CONFIG.N_STEPS_PER_EPOCH:\n                print(\n                    f'EPOCH {epoch+1:02d}, {step+1:04d}/{CONFIG.N_STEPS_PER_EPOCH} | ' + \n                    f'loss: {LOSS.avg:.4f}, mae: {MAE.compute().item():.4f}, r2: {R2.compute().item():.4f}, ' +\n                    f'step: {(time.perf_counter_ns()-t_start)*1e-9:.3f}s, lr: {LR_SCHEDULER.get_last_lr()[0]:.2e}',\n                )\n            elif CONFIG.IS_INTERACTIVE:\n                print(\n                    f'\\rEPOCH {epoch+1:02d}, {step+1:04d}/{CONFIG.N_STEPS_PER_EPOCH} | ' + \n                    f'loss: {LOSS.avg:.4f}, mae: {MAE.compute().item():.4f}, r2: {R2.compute().item():.4f}, ' +\n                    f'step: {(time.perf_counter_ns()-t_start)*1e-9:.3f}s, lr: {LR_SCHEDULER.get_last_lr()[0]:.2e}',\n                    end='\\n' if (step + 1) == CONFIG.N_STEPS_PER_EPOCH else '', flush=True,\n                )\n\n    torch.save(model, 'model.pth')\nelse:\n    model = torch.load('/kaggle/input/baseline-model/model.pth')","metadata":{"execution":{"iopub.status.busy":"2024-05-15T18:48:48.973123Z","iopub.status.idle":"2024-05-15T18:48:48.973588Z","shell.execute_reply.started":"2024-05-15T18:48:48.973352Z","shell.execute_reply":"2024-05-15T18:48:48.973371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluation of the model","metadata":{}},{"cell_type":"code","source":"if CONFIG.USE_VALID_SET is True:\n    model.eval()  # Set the model to evaluation mode\n    test_MAE = torchmetrics.regression.MeanAbsoluteError().to('cuda')\n    test_R2 = torchmetrics.regression.R2Score(num_outputs=CONFIG.N_TARGETS, multioutput='uniform_average').to('cuda')\n    test_loss = AverageMeter()\n\n    with torch.no_grad():  # Disable gradient computation for evaluation\n        for X_batch, y_true in valid_dataloader:\n            X_batch = X_batch.to('cuda')\n            y_true = y_true.to('cuda')\n\n            # Predict and evaluate\n            y_pred = model(X_batch)\n            loss = LOSS_FN(y_pred, y_true)\n            test_loss.update(loss)\n            test_MAE.update(y_pred, y_true)\n            test_R2.update(y_pred, y_true)\n\n    # Print test results\n    print(\n        f'Test Results - Loss: {test_loss.avg:.4f}, MAE: {test_MAE.compute().item():.4f}, ' +\n        f'R2: {test_R2.compute().item():.4f}'\n    )","metadata":{"execution":{"iopub.status.busy":"2024-05-15T18:48:48.974746Z","iopub.status.idle":"2024-05-15T18:48:48.975253Z","shell.execute_reply.started":"2024-05-15T18:48:48.974982Z","shell.execute_reply":"2024-05-15T18:48:48.975003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"SUBMISSION_ROWS = []\nmodel.eval()\n\nfor X_images_test,X_tabular_test, test_id in tqdm(test_dataset):\n    with torch.no_grad():\n        y_pred = model(X_images_test.unsqueeze(0).to('cuda'), X_tabular_test.unsqueeze(0).to('cuda')).detach().cpu().numpy()\n    \n    y_pred = SCALER.inverse_transform(y_pred).squeeze()\n    row = {'id': test_id}\n    \n    for k, v in zip(CONFIG.TARGET_COLUMNS, y_pred):\n        if k in LOG_FEATURES:\n            row[k.replace('_mean', '')] = 10 ** v\n        else:\n            row[k.replace('_mean', '')] = v\n\n    SUBMISSION_ROWS.append(row)\n    \nsubmission_df = pd.DataFrame(SUBMISSION_ROWS)\nsubmission_df.to_csv('submission.csv', index=False)\nprint(\"Submit!\")","metadata":{"execution":{"iopub.status.busy":"2024-05-15T18:48:48.976887Z","iopub.status.idle":"2024-05-15T18:48:48.977422Z","shell.execute_reply.started":"2024-05-15T18:48:48.977156Z","shell.execute_reply":"2024-05-15T18:48:48.977176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Submission with XGBoost","metadata":{"execution":{"iopub.status.busy":"2024-05-15T18:48:48.979130Z","iopub.status.idle":"2024-05-15T18:48:48.979611Z","shell.execute_reply.started":"2024-05-15T18:48:48.979364Z","shell.execute_reply":"2024-05-15T18:48:48.979384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Add the XGBoost model predictions","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport xgboost as xgb\nfrom tensorflow.keras.applications.resnet50 import preprocess_input\n# from tensorflow.keras.applications.efficientnet import preprocess_input\nfrom tensorflow.keras.applications import EfficientNetB0\n\nRESNET = False\nEFFICIENT = True\n\nif RESNET:\n    image_xy = [180,180]\nelif EFFICIENT:\n    image_xy = [300,300]\n    \nif CONFIG.USE_XGBOOST is True:\n#     train_features_xgb = pd.read_pickle('/kaggle/input/features-xgb/extracted_features.pkl')\n    train_features_xgb = pd.read_pickle('/kaggle/input/efficientnet/train_eff.pkl')\n    X_full = train_features_xgb.drop(columns=CONFIG.TARGET_COLUMNS+['id'])\n    Y_full = train_features_xgb[CONFIG.TARGET_COLUMNS]\n    models = {}\n    for column in Y_full.columns:\n        model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=250, learning_rate=0.038, max_depth=10)\n        model.load_model(f\"/kaggle/input/xgboost-finetuned-0-2393/model{column}.json\")\n        models[column] = model\n        image_model = EfficientNetB0(weights='imagenet', include_top=False, pooling='avg')\n\n    # Define the function to create a TensorFlow dataset for images\n    def create_dataset(image_paths, batch_size=128):\n        def process_path(file_path):\n            img = tf.io.read_file(file_path)\n            img = tf.image.decode_jpeg(img, channels=3)\n            img = tf.image.resize(img, image_xy)\n            if RESNET:\n                img = preprocess_input(img)\n            return img\n        path_ds = tf.data.Dataset.from_tensor_slices(image_paths)\n        image_ds = path_ds.map(process_path, num_parallel_calls=tf.data.AUTOTUNE)\n        image_ds = image_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n        return image_ds\n\n    def extract_features_with_dataset(dataset, df):\n        features_list = []\n        for batch_imgs in dataset:\n            print(\".\", end=\"\")  # Print progress\n            features = image_model.predict(batch_imgs, verbose=0)\n            features_list.extend(features)\n        features_array = np.array(features_list)\n\n        # Convert the features array into a DataFrame\n        features_df = pd.DataFrame(features_array)\n\n        features_df.columns = [f'feature_{i}' for i in range(features_array.shape[1])]\n\n        new_df = pd.concat([df.reset_index(drop=True), features_df.reset_index(drop=True)], axis=1)\n\n        return new_df\n\n    test_image_folder = '/kaggle/input/planttraits2024/test_images'\n    test = pd.read_csv('/kaggle/input/planttraits2024/test.csv')\n    image_paths = [os.path.join(test_image_folder, f\"{img_id}.jpeg\") for img_id in test['id']]\n\n    # Create the dataset\n    image_dataset = create_dataset(image_paths)\n\n    test_copy = test.drop(columns = 'id')\n    \n    # Extract features and directly insert them into the DataFrame as separate columns\n    test_XGB = extract_features_with_dataset(image_dataset, test_copy)\n\n    print(test_XGB.head())\n    mean_values = Y_full.mean()\n    submission = pd.DataFrame({'id': test['id']})\n    submission[Y_full.columns] = CONFIG.TARGET_COLUMNS\n\n    #rename from _mean\n    submission.columns = submission.columns.str.replace('_mean', '')\n    submission['X4'] = models['X4_mean'].predict(test_XGB)\n    submission['X11'] = models['X11_mean'].predict(test_XGB)\n    submission['X18'] = models['X18_mean'].predict(test_XGB)\n    submission['X50'] = models['X50_mean'].predict(test_XGB)\n    submission['X26'] = models['X26_mean'].predict(test_XGB)\n    submission['X3112'] = models['X3112_mean'].predict(test_XGB)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T18:48:48.981129Z","iopub.status.idle":"2024-05-15T18:48:48.981625Z","shell.execute_reply.started":"2024-05-15T18:48:48.981366Z","shell.execute_reply":"2024-05-15T18:48:48.981386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if CONFIG.USE_XGBOOST is True:\n    combined_submission = pd.DataFrame()\n    combined_submission['id'] = submission['id']\n    for column in Y_full.columns.str.replace('_mean', ''):\n        combined_submission[column] = submission[column]*0.3 + submission_df[column]*0.7\n\n    combined_submission.to_csv('submission.csv', index=False)\n    print(\"submission ready!\")\n    combined_submission","metadata":{"execution":{"iopub.status.busy":"2024-05-15T18:48:48.983559Z","iopub.status.idle":"2024-05-15T18:48:48.984036Z","shell.execute_reply.started":"2024-05-15T18:48:48.983788Z","shell.execute_reply":"2024-05-15T18:48:48.983808Z"},"trusted":true},"execution_count":null,"outputs":[]}]}