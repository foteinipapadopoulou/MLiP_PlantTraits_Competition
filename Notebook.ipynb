{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":65626,"databundleVersionId":8046133,"sourceType":"competition"},{"sourceId":8145845,"sourceType":"datasetVersion","datasetId":4816952},{"sourceId":8149784,"sourceType":"datasetVersion","datasetId":4819871},{"sourceId":8178456,"sourceType":"datasetVersion","datasetId":4841404},{"sourceId":8204134,"sourceType":"datasetVersion","datasetId":4860734},{"sourceId":8206880,"sourceType":"datasetVersion","datasetId":4862895}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"- Notebook modified from https://www.kaggle.com/code/markwijkhuizen/planttraits2024-eda-training-pub.\n- Training only, EDA part not included.\n- Image model only, tabular data not used.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport imageio.v3 as imageio\nimport albumentations as A\n\nfrom albumentations.pytorch import ToTensorV2\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch import nn\nfrom tqdm.notebook import tqdm\nfrom sklearn.preprocessing import StandardScaler\n\nimport torch\nimport timm\nimport glob\nimport torchmetrics\nimport time\nimport psutil\nimport os\n\ntqdm.pandas()","metadata":{"execution":{"iopub.status.busy":"2024-04-23T16:26:49.618096Z","iopub.execute_input":"2024-04-23T16:26:49.618486Z","iopub.status.idle":"2024-04-23T16:26:49.625634Z","shell.execute_reply.started":"2024-04-23T16:26:49.618450Z","shell.execute_reply":"2024-04-23T16:26:49.624609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Config():\n    IMAGE_SIZE = 384\n    BACKBONE = 'swin_large_patch4_window12_384.ms_in22k_ft_in1k'\n    TARGET_COLUMNS = ['X4_mean', 'X11_mean', 'X18_mean', 'X50_mean', 'X26_mean', 'X3112_mean']\n    N_TARGETS = len(TARGET_COLUMNS)\n    BATCH_SIZE = 10\n    LR_MAX = 1e-4\n    WEIGHT_DECAY = 0.01\n    N_EPOCHS = 6\n    TRAIN_MODEL = False\n    IS_INTERACTIVE = os.environ['KAGGLE_KERNEL_RUN_TYPE'] == 'Interactive'\n    BUILD_PKL_DATASET = False\n    BUILD_VALID_SET = False\n    USE_SMALL_DATASET = False\n    USE_VALID_SET = False\n    USE_MODIFIED_TRAIN = False\n        \nCONFIG = Config()","metadata":{"execution":{"iopub.status.busy":"2024-04-23T16:26:51.091132Z","iopub.execute_input":"2024-04-23T16:26:51.091649Z","iopub.status.idle":"2024-04-23T16:26:51.098616Z","shell.execute_reply.started":"2024-04-23T16:26:51.091611Z","shell.execute_reply":"2024-04-23T16:26:51.097634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if CONFIG.BUILD_PKL_DATASET is True:\n    train = pd.read_csv('/kaggle/input/planttraits2024/train.csv')\n    train['file_path'] = train['id'].apply(lambda s: f'/kaggle/input/planttraits2024/train_images/{s}.jpeg')\n    train['jpeg_bytes'] = train['file_path'].progress_apply(lambda fp: open(fp, 'rb').read())\n    train.to_pickle('train.pkl')\n    test = pd.read_csv('/kaggle/input/planttraits2024/test.csv')\n    test['file_path'] = test['id'].apply(lambda s: f'/kaggle/input/planttraits2024/test_images/{s}.jpeg')\n    test['jpeg_bytes'] = test['file_path'].progress_apply(lambda fp: open(fp, 'rb').read())\n    test.to_pickle('test.pkl')\nelse:\n    if CONFIG.USE_SMALL_DATASET is True:\n        train = pd.read_pickle('/kaggle/input/dataset-with-validation/small_train.pkl')       \n    elif CONFIG.USE_MODIFIED_TRAIN is True:\n        train = pd.read_pickle('/kaggle/input/dataset-with-validation/train_set.pkl')\n    else:\n         train = pd.read_pickle('/kaggle/input/baseline-model/train.pkl')\n    test = pd.read_pickle('/kaggle/input/baseline-model/test.pkl')\n\nfor column in CONFIG.TARGET_COLUMNS:\n    lower_quantile = train[column].quantile(0.005)\n    upper_quantile = train[column].quantile(0.985)  \n    train = train[(train[column] >= lower_quantile) & (train[column] <= upper_quantile)]\n\nCONFIG.N_TRAIN_SAMPLES = len(train)\nCONFIG.N_STEPS_PER_EPOCH = (CONFIG.N_TRAIN_SAMPLES // CONFIG.BATCH_SIZE)\nCONFIG.N_STEPS = CONFIG.N_STEPS_PER_EPOCH * CONFIG.N_EPOCHS + 1\n\nprint('N_TRAIN_SAMPLES:', len(train), 'N_TEST_SAMPLES:', len(test))","metadata":{"execution":{"iopub.status.busy":"2024-04-23T16:27:05.358227Z","iopub.execute_input":"2024-04-23T16:27:05.359087Z","iopub.status.idle":"2024-04-23T16:27:34.473615Z","shell.execute_reply.started":"2024-04-23T16:27:05.359050Z","shell.execute_reply":"2024-04-23T16:27:34.472473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#all columns must be identical to be consider the same species\ntrait_columns = ['X4_mean', 'X11_mean', 'X18_mean', 'X50_mean', 'X26_mean', 'X3112_mean']\ntrain['species'] = train.groupby(CONFIG.TARGET_COLUMNS).ngroup()\nspecies_counts = train['species'].nunique()\nprint (f\"{species_counts} unique species found in {len(train)} records\")","metadata":{"execution":{"iopub.status.busy":"2024-04-23T16:27:34.475030Z","iopub.execute_input":"2024-04-23T16:27:34.475326Z","iopub.status.idle":"2024-04-23T16:27:34.515443Z","shell.execute_reply.started":"2024-04-23T16:27:34.475301Z","shell.execute_reply":"2024-04-23T16:27:34.514607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"species_counts = train['species'].value_counts()\ncount_summary = species_counts.value_counts().sort_index()\ncount_summary.plot(kind='bar', figsize=(7, 3))\n\nplt.title('Images per Species')\nplt.xlabel('Images')\nplt.ylabel('Species')\nplt.tight_layout() \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-23T16:27:47.928278Z","iopub.execute_input":"2024-04-23T16:27:47.928940Z","iopub.status.idle":"2024-04-23T16:27:48.332945Z","shell.execute_reply.started":"2024-04-23T16:27:47.928907Z","shell.execute_reply":"2024-04-23T16:27:48.332024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if CONFIG.BUILD_VALID_SET is True:\n    unique_species = train['species'].unique()\n    n_test_species_exclusive = 2\n\n    # Randomly pick exclusive test species\n    test_species_exclusive = np.random.choice(unique_species, n_test_species_exclusive, replace=False)\n\n    # Isolate exclusive test species data\n    exclusive_test_set = train[train['species'].isin(test_species_exclusive)]\n\n    # Remove exclusive test species from main data\n    train_reduced = train[~train['species'].isin(test_species_exclusive)]\n    print(len(train_reduced))\n    test_set_common = pd.DataFrame()\n    for species in train_reduced['species'].unique():\n        species_data = train_reduced[train_reduced['species'] == species]\n        sample = species_data.sample(1, random_state=42) \n        test_set_common = pd.concat([test_set_common, sample])\n\n    # Remove these samples from train\n    train_set = train_reduced.drop(test_set_common.index)\n\n    valid_set = pd.concat([exclusive_test_set, test_set_common])\n#     train_set.to_pickle('train_set.pkl')\n#     valid_set.to_pickle('valid_set.pkl')\n    \n    print(f\"Training set size: {len(train_set)}\")\n    print(f\"Valid set size: {len(final_test_set)}\")\n    print(f\"Unique species in Training set: {train_set['species'].nunique()}\")\n    print(f\"Unique species in Valid set: {final_test_set['species'].nunique()}\")\n    train = train_set\nelif CONFIG.USE_VALID_SET is True:\n    valid_set = pd.read_pickle('/kaggle/input/dataset-with-validation/valid_set.pkl')\n    ","metadata":{"execution":{"iopub.status.busy":"2024-04-23T16:27:51.831507Z","iopub.execute_input":"2024-04-23T16:27:51.831928Z","iopub.status.idle":"2024-04-23T16:27:51.841094Z","shell.execute_reply.started":"2024-04-23T16:27:51.831899Z","shell.execute_reply":"2024-04-23T16:27:51.840205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_small_dataset(train):\n    small_dataset = train.sample(n=10000)\n    small_dataset.to_pickle('small_train.pkl')\n# create_small_dataset(train_set)","metadata":{"execution":{"iopub.status.busy":"2024-04-20T21:17:26.780123Z","iopub.execute_input":"2024-04-20T21:17:26.782131Z","iopub.status.idle":"2024-04-20T21:17:31.700265Z","shell.execute_reply.started":"2024-04-20T21:17:26.782081Z","shell.execute_reply":"2024-04-20T21:17:31.699110Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LOG_FEATURES = ['X4_mean', 'X11_mean', 'X18_mean', 'X50_mean', 'X26_mean', 'X3112_mean']\n\ny_train = np.zeros_like(train[CONFIG.TARGET_COLUMNS], dtype=np.float32)\nfor target_idx, target in enumerate(CONFIG.TARGET_COLUMNS):\n    v = train[target].values\n    if target in LOG_FEATURES:\n        v = np.log10(v)\n    y_train[:, target_idx] = v\n\nSCALER = StandardScaler()\ny_train = SCALER.fit_transform(y_train)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T16:28:03.459748Z","iopub.execute_input":"2024-04-23T16:28:03.460392Z","iopub.status.idle":"2024-04-23T16:28:03.483428Z","shell.execute_reply.started":"2024-04-23T16:28:03.460360Z","shell.execute_reply":"2024-04-23T16:28:03.482578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MEAN = np.array([0.485, 0.456, 0.406])\nSTD = np.array([0.229, 0.224, 0.225])\n\nTRAIN_TRANSFORMS = A.Compose([\n        A.HorizontalFlip(p=0.5),\n        A.RandomSizedCrop(\n            [448, 512],\n            CONFIG.IMAGE_SIZE, CONFIG.IMAGE_SIZE, w2h_ratio=1.0, p=0.75),\n        A.Resize(CONFIG.IMAGE_SIZE, CONFIG.IMAGE_SIZE),\n        A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.25),\n        A.ImageCompression(quality_lower=85, quality_upper=100, p=0.25),\n        A.ToFloat(),\n        A.Normalize(mean=MEAN, std=STD, max_pixel_value=1),\n        ToTensorV2(),\n    ])\n\nTEST_TRANSFORMS = A.Compose([\n        A.Resize(CONFIG.IMAGE_SIZE, CONFIG.IMAGE_SIZE),\n        A.ToFloat(),\n        A.Normalize(mean=MEAN, std=STD, max_pixel_value=1),\n        ToTensorV2(),\n    ])\n\nclass Dataset(Dataset):\n    def __init__(self, X_jpeg_bytes, y, transforms=None):\n        self.X_jpeg_bytes = X_jpeg_bytes\n        self.y = y\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.X_jpeg_bytes)\n\n    def __getitem__(self, index):\n        X_sample = self.transforms(\n            image=imageio.imread(self.X_jpeg_bytes[index]),\n        )['image']\n\n        y_sample = self.y[index]\n  \n        return X_sample, y_sample\n\n    \nif CONFIG.USE_VALID_SET is True:\n    y_valid = np.zeros_like(valid_set[CONFIG.TARGET_COLUMNS], dtype=np.float32)\n    for target_idx, target in enumerate(CONFIG.TARGET_COLUMNS):\n        v = valid_set[target].values\n        if target in LOG_FEATURES:\n            v = np.log10(v)\n        y_valid[:, target_idx] = v\n\n    SCALER = StandardScaler()\n    y_valid = SCALER.fit_transform(y_valid)\n    valid_dataset = Dataset( \n        valid_set['jpeg_bytes'].values,\n        y_valid,\n        TEST_TRANSFORMS,\n    )\n    valid_dataloader = DataLoader(\n        valid_dataset,\n        shuffle=False,\n        batch_size=CONFIG.BATCH_SIZE,\n        num_workers=psutil.cpu_count(),\n)\n\n    \ntrain_dataset = Dataset(\n    train['jpeg_bytes'].values,\n    y_train,\n    TRAIN_TRANSFORMS,\n)\n\ntrain_dataloader = DataLoader(\n        train_dataset,\n        batch_size=CONFIG.BATCH_SIZE,\n        shuffle=True,\n        drop_last=True,\n        num_workers=psutil.cpu_count(),\n)\n\ntest_dataset = Dataset(\n    test['jpeg_bytes'].values,\n    test['id'].values,\n    TEST_TRANSFORMS,\n)   ","metadata":{"execution":{"iopub.status.busy":"2024-04-23T16:28:05.251805Z","iopub.execute_input":"2024-04-23T16:28:05.252435Z","iopub.status.idle":"2024-04-23T16:28:05.270764Z","shell.execute_reply.started":"2024-04-23T16:28:05.252394Z","shell.execute_reply":"2024-04-23T16:28:05.269583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.backbone = timm.create_model(\n                CONFIG.BACKBONE,\n                num_classes=CONFIG.N_TARGETS,\n                pretrained=True)\n        \n    def forward(self, inputs):\n        return self.backbone(inputs)\n\nmodel = Model()\nmodel = model.to('cuda')\n# print(model)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T16:28:07.808135Z","iopub.execute_input":"2024-04-23T16:28:07.809013Z","iopub.status.idle":"2024-04-23T16:28:15.013290Z","shell.execute_reply.started":"2024-04-23T16:28:07.808978Z","shell.execute_reply":"2024-04-23T16:28:15.012495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_lr_scheduler(optimizer):\n    return torch.optim.lr_scheduler.OneCycleLR(\n        optimizer=optimizer,\n        max_lr=CONFIG.LR_MAX,\n        total_steps=CONFIG.N_STEPS,\n        pct_start=0.1,\n        anneal_strategy='cos',\n        div_factor=1e1,\n        final_div_factor=1e1,\n    )\n\nclass AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val):\n        self.sum += val.sum()\n        self.count += val.numel()\n        self.avg = self.sum / self.count\n\nMAE = torchmetrics.regression.MeanAbsoluteError().to('cuda')\nR2 = torchmetrics.regression.R2Score(num_outputs=CONFIG.N_TARGETS, multioutput='uniform_average').to('cuda')\nLOSS = AverageMeter()\n\nY_MEAN = torch.tensor(y_train).mean(dim=0).to('cuda')\nEPS = torch.tensor([1e-6]).to('cuda')\n\ndef r2_loss(y_pred, y_true):\n    ss_res = torch.sum((y_true - y_pred)**2, dim=0)\n    ss_total = torch.sum((y_true - Y_MEAN)**2, dim=0)\n    ss_total = torch.maximum(ss_total, EPS)\n    r2 = torch.mean(ss_res / ss_total)\n    return r2\n\nLOSS_FN = nn.SmoothL1Loss() # r2_loss\n\noptimizer = torch.optim.AdamW(\n    params=model.parameters(),\n    lr=CONFIG.LR_MAX,\n    weight_decay=CONFIG.WEIGHT_DECAY,\n)\n\nLR_SCHEDULER = get_lr_scheduler(optimizer)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T16:28:15.015059Z","iopub.execute_input":"2024-04-23T16:28:15.015361Z","iopub.status.idle":"2024-04-23T16:28:15.035165Z","shell.execute_reply.started":"2024-04-23T16:28:15.015334Z","shell.execute_reply":"2024-04-23T16:28:15.034244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if CONFIG.TRAIN_MODEL is True:\n    print(\"Start Training:\")\n    for epoch in range(CONFIG.N_EPOCHS):\n        MAE.reset()\n        R2.reset()\n        LOSS.reset()\n        model.train()\n\n        for step, (X_batch, y_true) in enumerate(train_dataloader):\n            X_batch = X_batch.to('cuda')\n            y_true = y_true.to('cuda')\n            t_start = time.perf_counter_ns()\n            y_pred = model(X_batch)\n            loss = LOSS_FN(y_pred, y_true)\n            LOSS.update(loss)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n            LR_SCHEDULER.step()\n            MAE.update(y_pred, y_true)\n            R2.update(y_pred, y_true)\n\n            if not CONFIG.IS_INTERACTIVE and (step+1) == CONFIG.N_STEPS_PER_EPOCH:\n                print(\n                    f'EPOCH {epoch+1:02d}, {step+1:04d}/{CONFIG.N_STEPS_PER_EPOCH} | ' + \n                    f'loss: {LOSS.avg:.4f}, mae: {MAE.compute().item():.4f}, r2: {R2.compute().item():.4f}, ' +\n                    f'step: {(time.perf_counter_ns()-t_start)*1e-9:.3f}s, lr: {LR_SCHEDULER.get_last_lr()[0]:.2e}',\n                )\n            elif CONFIG.IS_INTERACTIVE:\n                print(\n                    f'\\rEPOCH {epoch+1:02d}, {step+1:04d}/{CONFIG.N_STEPS_PER_EPOCH} | ' + \n                    f'loss: {LOSS.avg:.4f}, mae: {MAE.compute().item():.4f}, r2: {R2.compute().item():.4f}, ' +\n                    f'step: {(time.perf_counter_ns()-t_start)*1e-9:.3f}s, lr: {LR_SCHEDULER.get_last_lr()[0]:.2e}',\n                    end='\\n' if (step + 1) == CONFIG.N_STEPS_PER_EPOCH else '', flush=True,\n                )\n\n    torch.save(model, 'model.pth')\nelse:\n    model = torch.load('/kaggle/input/baseline-model/model.pth')","metadata":{"execution":{"iopub.status.busy":"2024-04-23T16:28:15.036308Z","iopub.execute_input":"2024-04-23T16:28:15.036697Z","iopub.status.idle":"2024-04-23T16:28:24.426546Z","shell.execute_reply.started":"2024-04-23T16:28:15.036674Z","shell.execute_reply":"2024-04-23T16:28:24.425772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluation of the model","metadata":{}},{"cell_type":"code","source":"# model.eval()  # Set the model to evaluation mode\n# test_MAE = torchmetrics.regression.MeanAbsoluteError().to('cuda')\n# test_R2 = torchmetrics.regression.R2Score(num_outputs=CONFIG.N_TARGETS, multioutput='uniform_average').to('cuda')\n# test_loss = AverageMeter()\n\n# with torch.no_grad():  # Disable gradient computation for evaluation\n#     for X_batch, y_true in valid_dataloader:\n#         X_batch = X_batch.to('cuda')\n#         y_true = y_true.to('cuda')\n        \n#         # Predict and evaluate\n#         y_pred = model(X_batch)\n#         loss = LOSS_FN(y_pred, y_true)\n#         test_loss.update(loss)\n#         test_MAE.update(y_pred, y_true)\n#         test_R2.update(y_pred, y_true)\n\n# # Print test results\n# print(\n#     f'Test Results - Loss: {test_loss.avg:.4f}, MAE: {test_MAE.compute().item():.4f}, ' +\n#     f'R2: {test_R2.compute().item():.4f}'\n# )","metadata":{"execution":{"iopub.status.busy":"2024-04-23T14:55:48.033987Z","iopub.execute_input":"2024-04-23T14:55:48.034405Z","iopub.status.idle":"2024-04-23T14:55:48.722300Z","shell.execute_reply.started":"2024-04-23T14:55:48.034373Z","shell.execute_reply":"2024-04-23T14:55:48.720849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SUBMISSION_ROWS = []\nmodel.eval()\n\nfor X_sample_test, test_id in tqdm(test_dataset):\n    with torch.no_grad():\n        y_pred = model(X_sample_test.unsqueeze(0).to('cuda')).detach().cpu().numpy()\n    \n    y_pred = SCALER.inverse_transform(y_pred).squeeze()\n    row = {'id': test_id}\n    \n    for k, v in zip(CONFIG.TARGET_COLUMNS, y_pred):\n        if k in LOG_FEATURES:\n            row[k.replace('_mean', '')] = 10 ** v\n        else:\n            row[k.replace('_mean', '')] = v\n\n    SUBMISSION_ROWS.append(row)\n    \nsubmission_df = pd.DataFrame(SUBMISSION_ROWS)\n#submission_df.to_csv('submission.csv', index=False)\n#print(\"Submit!\")","metadata":{"execution":{"iopub.status.busy":"2024-04-23T16:28:30.491949Z","iopub.execute_input":"2024-04-23T16:28:30.492785Z","iopub.status.idle":"2024-04-23T16:34:34.217268Z","shell.execute_reply.started":"2024-04-23T16:28:30.492754Z","shell.execute_reply":"2024-04-23T16:34:34.216301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Add the XGBoost model predictions","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import xgboost as xgb\ntrain_features_xgb = pd.read_pickle('/kaggle/input/features-xgb/extracted_features.pkl')\n\nX_full = train_features_xgb.drop(columns=CONFIG.TARGET_COLUMNS)\nY_full = train_features_xgb[CONFIG.TARGET_COLUMNS]\nmodels = {}\nfor column in Y_full.columns:\n    model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=150, learning_rate=0.1, max_depth=10)\n    model.load_model(f\"/kaggle/input/xgb-models/model{column}.json\")\n    models[column] = model","metadata":{"execution":{"iopub.status.busy":"2024-04-23T16:51:36.479963Z","iopub.execute_input":"2024-04-23T16:51:36.480599Z","iopub.status.idle":"2024-04-23T16:51:38.839942Z","shell.execute_reply.started":"2024-04-23T16:51:36.480552Z","shell.execute_reply":"2024-04-23T16:51:38.839160Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.applications.resnet50 import preprocess_input\nfrom tensorflow.keras.applications import EfficientNetB0\nimage_model = EfficientNetB0(weights='imagenet', include_top=False, pooling='avg')\n\n# Define the function to create a TensorFlow dataset for images\ndef create_dataset(image_paths, batch_size=128):\n    def process_path(file_path):\n        img = tf.io.read_file(file_path)\n        img = tf.image.decode_jpeg(img, channels=3)\n        img = tf.image.resize(img, [224, 224])\n        img = preprocess_input(img)\n        return img\n    path_ds = tf.data.Dataset.from_tensor_slices(image_paths)\n    image_ds = path_ds.map(process_path, num_parallel_calls=tf.data.AUTOTUNE)\n    image_ds = image_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n    return image_ds\n\ndef extract_features_with_dataset(dataset, df):\n    features_list = []\n    for batch_imgs in dataset:\n        print(\".\", end=\"\")  # Print progress\n        features = image_model.predict(batch_imgs, verbose=0)\n        features_list.extend(features)\n    features_array = np.array(features_list)\n    \n    # Convert the features array into a DataFrame\n    features_df = pd.DataFrame(features_array)\n    \n    features_df.columns = [f'feature_{i}' for i in range(features_array.shape[1])]\n    \n    new_df = pd.concat([df.reset_index(drop=True), features_df.reset_index(drop=True)], axis=1)\n    \n    return new_df\n\ntest_image_folder = '/kaggle/input/planttraits2024/test_images'\n\nimage_paths = [os.path.join(test_image_folder, f\"{img_id}.jpeg\") for img_id in test['id']]\n\n# Create the dataset\nimage_dataset = create_dataset(image_paths)\ntest = pd.read_csv('/kaggle/input/planttraits2024/test.csv')\n# Extract features and directly insert them into the DataFrame as separate columns\ntest_XGB = extract_features_with_dataset(image_dataset, test)\n\nprint(test_XGB.head())","metadata":{"execution":{"iopub.status.busy":"2024-04-23T16:54:01.262446Z","iopub.execute_input":"2024-04-23T16:54:01.263705Z","iopub.status.idle":"2024-04-23T16:54:27.521061Z","shell.execute_reply.started":"2024-04-23T16:54:01.263662Z","shell.execute_reply":"2024-04-23T16:54:27.519113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean_values = Y_full.mean()\nsubmission = pd.DataFrame({'id': test['id']})\nsubmission[Y_full.columns] = CONFIG.TARGET_COLUMNS\n\n#rename from _mean\nsubmission.columns = submission.columns.str.replace('_mean', '')\nsubmission['X4'] = models['X4_mean'].predict(test_XGB)\nsubmission['X11'] = models['X11_mean'].predict(test_XGB)\nsubmission['X18'] = models['X18_mean'].predict(test_XGB)\nsubmission['X50'] = models['X50_mean'].predict(test_XGB)\nsubmission['X26'] = models['X26_mean'].predict(test_XGB)\nsubmission['X3112'] = models['X3112_mean'].predict(test_XGB)","metadata":{"execution":{"iopub.status.busy":"2024-04-23T16:59:51.122289Z","iopub.execute_input":"2024-04-23T16:59:51.123040Z","iopub.status.idle":"2024-04-23T16:59:53.017999Z","shell.execute_reply.started":"2024-04-23T16:59:51.123010Z","shell.execute_reply":"2024-04-23T16:59:53.017180Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Averaging predictions\ncombined_submission = pd.DataFrame()\ncombined_submission['id'] = submission['id']\nfor column in Y_full.columns.str.replace('_mean', ''):\n    combined_submission[column] = submission[column]*0.1 + submission_df[column]*0.9\n\ncombined_submission.to_csv('submission.csv', index=False)\nprint(\"submission ready!\")","metadata":{"execution":{"iopub.status.busy":"2024-04-23T16:57:12.626492Z","iopub.execute_input":"2024-04-23T16:57:12.626859Z","iopub.status.idle":"2024-04-23T16:57:12.729236Z","shell.execute_reply.started":"2024-04-23T16:57:12.626833Z","shell.execute_reply":"2024-04-23T16:57:12.728285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combined_submission","metadata":{"execution":{"iopub.status.busy":"2024-04-23T16:58:02.569423Z","iopub.execute_input":"2024-04-23T16:58:02.570083Z","iopub.status.idle":"2024-04-23T16:58:02.585772Z","shell.execute_reply.started":"2024-04-23T16:58:02.570051Z","shell.execute_reply":"2024-04-23T16:58:02.584843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}