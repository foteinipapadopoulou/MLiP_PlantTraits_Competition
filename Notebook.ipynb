{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":65626,"databundleVersionId":8046133,"sourceType":"competition"},{"sourceId":8145845,"sourceType":"datasetVersion","datasetId":4816952},{"sourceId":8149784,"sourceType":"datasetVersion","datasetId":4819871},{"sourceId":8178456,"sourceType":"datasetVersion","datasetId":4841404},{"sourceId":8204134,"sourceType":"datasetVersion","datasetId":4860734},{"sourceId":8332370,"sourceType":"datasetVersion","datasetId":4862895},{"sourceId":8332380,"sourceType":"datasetVersion","datasetId":4947895},{"sourceId":8409225,"sourceType":"datasetVersion","datasetId":4912621}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Train image models Notebook","metadata":{}},{"cell_type":"markdown","source":"This notebook includes the training of image models, Swin and EVA transformers.\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport imageio.v3 as imageio\nimport albumentations as A\n\nfrom albumentations.pytorch import ToTensorV2\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch import nn\nfrom tqdm.notebook import tqdm\nfrom sklearn.preprocessing import StandardScaler\n\nimport torch\nimport timm\nimport glob\nimport torchmetrics\nimport time\nimport psutil\nimport os\n\ntqdm.pandas()","metadata":{"execution":{"iopub.status.busy":"2024-06-03T10:10:42.022774Z","iopub.execute_input":"2024-06-03T10:10:42.023443Z","iopub.status.idle":"2024-06-03T10:10:52.878253Z","shell.execute_reply.started":"2024-06-03T10:10:42.023411Z","shell.execute_reply":"2024-06-03T10:10:52.877419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Config():\n    IMAGE_SIZE = 196# 256 # 384\n    BACKBONE = 'eva_large_patch14_196.in22k_ft_in22k_in1k'#'convnext_base.clip_laion2b_augreg_ft_in12k_in1k' # 'caformer_b36.sail_in22k_ft_in1k_384' # 'swin_large_patch4_window12_384.ms_in22k_ft_in1k'\n    TARGET_COLUMNS = ['X4_mean', 'X11_mean', 'X18_mean', 'X50_mean', 'X26_mean', 'X3112_mean']\n    N_TARGETS = len(TARGET_COLUMNS)\n    TRAIN_MODEL = True\n    BUILD_PKL_DATASET = False\n    EXTRACT_FEATURES = False\n    BATCH_SIZE = 10\n    LR_MAX = 1e-4\n    WEIGHT_DECAY = 0.01\n    N_EPOCHS = 6\n    TRAIN_MODEL = True\n    IS_INTERACTIVE = os.environ['KAGGLE_KERNEL_RUN_TYPE'] == 'Interactive'\n        \nCONFIG = Config()","metadata":{"execution":{"iopub.status.busy":"2024-06-03T10:10:52.879886Z","iopub.execute_input":"2024-06-03T10:10:52.880265Z","iopub.status.idle":"2024-06-03T10:10:52.886611Z","shell.execute_reply.started":"2024-06-03T10:10:52.880234Z","shell.execute_reply":"2024-06-03T10:10:52.885525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load the dataset and preprocessing","metadata":{}},{"cell_type":"code","source":"if CONFIG.BUILD_PKL_DATASET is True:\n    train = pd.read_csv('/kaggle/input/planttraits2024/train.csv')\n    train['file_path'] = train['id'].apply(lambda s: f'/kaggle/input/planttraits2024/train_images/{s}.jpeg')\n    train['jpeg_bytes'] = train['file_path'].progress_apply(lambda fp: open(fp, 'rb').read())\n    train.to_pickle('train.pkl')\n    test = pd.read_csv('/kaggle/input/planttraits2024/test.csv')\n    test['file_path'] = test['id'].apply(lambda s: f'/kaggle/input/planttraits2024/test_images/{s}.jpeg')\n    test['jpeg_bytes'] = test['file_path'].progress_apply(lambda fp: open(fp, 'rb').read())\n    test.to_pickle('test.pkl')\nelse:\n    train = pd.read_pickle('/kaggle/input/baseline-model/train.pkl')\n    test = pd.read_pickle('/kaggle/input/baseline-model/test.pkl')\n    \nfor column in CONFIG.TARGET_COLUMNS:\n    lower_quantile = train[column].quantile(0.005)\n    upper_quantile = train[column].quantile(0.985)  \n    train = train[(train[column] >= lower_quantile) & (train[column] <= upper_quantile)]\n\nCONFIG.N_TRAIN_SAMPLES = len(train)\nCONFIG.N_STEPS_PER_EPOCH = (CONFIG.N_TRAIN_SAMPLES // CONFIG.BATCH_SIZE)\nCONFIG.N_STEPS = CONFIG.N_STEPS_PER_EPOCH * CONFIG.N_EPOCHS + 1\n\nprint('N_TRAIN_SAMPLES:', len(train), 'N_TEST_SAMPLES:', len(test))","metadata":{"execution":{"iopub.status.busy":"2024-06-03T09:47:21.640197Z","iopub.execute_input":"2024-06-03T09:47:21.640421Z","iopub.status.idle":"2024-06-03T09:47:57.844945Z","shell.execute_reply.started":"2024-06-03T09:47:21.640402Z","shell.execute_reply":"2024-06-03T09:47:57.843913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LOG_FEATURES = ['X4_mean', 'X11_mean', 'X18_mean', 'X50_mean', 'X26_mean', 'X3112_mean']\n\ny_train = np.zeros_like(train[CONFIG.TARGET_COLUMNS], dtype=np.float32)\nfor target_idx, target in enumerate(CONFIG.TARGET_COLUMNS):\n    v = train[target].values\n    if target in LOG_FEATURES:\n        v = np.log10(v)\n    y_train[:, target_idx] = v\n\nSCALER = StandardScaler()\ny_train = SCALER.fit_transform(y_train)","metadata":{"execution":{"iopub.status.busy":"2024-06-03T09:58:23.319492Z","iopub.execute_input":"2024-06-03T09:58:23.320245Z","iopub.status.idle":"2024-06-03T09:58:23.343409Z","shell.execute_reply.started":"2024-06-03T09:58:23.320210Z","shell.execute_reply":"2024-06-03T09:58:23.342653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create the transforms for the train and the test datasets and make the dataloaders","metadata":{}},{"cell_type":"code","source":"MEAN = np.array([0.485, 0.456, 0.406])\nSTD = np.array([0.229, 0.224, 0.225])\n\nTRAIN_TRANSFORMS = A.Compose([\n        A.HorizontalFlip(p=0.5),\n        A.RandomSizedCrop(\n            [448, 512],\n            CONFIG.IMAGE_SIZE, CONFIG.IMAGE_SIZE, w2h_ratio=1.0, p=0.75),\n        A.Resize(CONFIG.IMAGE_SIZE, CONFIG.IMAGE_SIZE),\n        A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.25),\n        A.ImageCompression(quality_lower=85, quality_upper=100, p=0.25),\n        A.ToFloat(),\n        A.Normalize(mean=MEAN, std=STD, max_pixel_value=1),\n        ToTensorV2(),\n    ])\n\nTEST_TRANSFORMS = A.Compose([\n        A.Resize(CONFIG.IMAGE_SIZE, CONFIG.IMAGE_SIZE),\n        A.ToFloat(),\n        A.Normalize(mean=MEAN, std=STD, max_pixel_value=1),\n        ToTensorV2(),\n    ])\n\nclass Dataset(Dataset):\n    def __init__(self, X_jpeg_bytes, y, transforms=None):\n        self.X_jpeg_bytes = X_jpeg_bytes\n        self.y = y\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.X_jpeg_bytes)\n\n    def __getitem__(self, index):\n        X_sample = self.transforms(\n            image=imageio.imread(self.X_jpeg_bytes[index]),\n        )['image']\n        y_sample = self.y[index]\n        \n        return X_sample, y_sample\n\ntrain_dataset = Dataset(\n    train['jpeg_bytes'].values,\n    y_train,\n    TRAIN_TRANSFORMS,\n)\n\ntrain_dataloader = DataLoader(\n        train_dataset,\n        batch_size=CONFIG.BATCH_SIZE,\n        shuffle=True,\n        drop_last=True,\n        num_workers=psutil.cpu_count(),\n)\n\ntest_dataset = Dataset(\n    test['jpeg_bytes'].values,\n    test['id'].values,\n    TEST_TRANSFORMS,\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-03T09:58:24.032328Z","iopub.execute_input":"2024-06-03T09:58:24.033166Z","iopub.status.idle":"2024-06-03T09:58:24.045847Z","shell.execute_reply.started":"2024-06-03T09:58:24.033136Z","shell.execute_reply":"2024-06-03T09:58:24.044894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create a model to make the predictions based on a pretrained one","metadata":{"execution":{"iopub.status.busy":"2024-06-04T11:00:11.324619Z","iopub.execute_input":"2024-06-04T11:00:11.324959Z","iopub.status.idle":"2024-06-04T11:00:11.329865Z","shell.execute_reply.started":"2024-06-04T11:00:11.324922Z","shell.execute_reply":"2024-06-04T11:00:11.328863Z"}}},{"cell_type":"code","source":"class Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.backbone = timm.create_model(\n            CONFIG.BACKBONE,\n            num_classes=CONFIG.N_TARGETS,\n            pretrained=True,\n        )\n        \n    def forward(self, inputs):\n        return self.backbone(inputs)","metadata":{"execution":{"iopub.status.busy":"2024-06-03T09:58:25.858476Z","iopub.execute_input":"2024-06-03T09:58:25.859061Z","iopub.status.idle":"2024-06-03T09:58:25.864796Z","shell.execute_reply.started":"2024-06-03T09:58:25.859031Z","shell.execute_reply":"2024-06-03T09:58:25.863739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# A model where we freeze the first couple of layers","metadata":{}},{"cell_type":"code","source":"class FreezeModel(nn.Module):\n    def __init__(self, freeze_till_stage=2):\n        super().__init__()\n        self.backbone = timm.create_model(\n            CONFIG.BACKBONE,\n            num_classes=0, \n            pretrained=True\n        )\n    \n        # Freeze layers up \n        for name, param in self.backbone.named_parameters():\n            if 'layers.' in name and int(name.split('.')[1]) < freeze_till_stage:\n                param.requires_grad = False\n        self.classifier = nn.Linear(self.backbone.num_features, CONFIG.N_TARGETS)\n\n    def forward(self, inputs):\n        x = self.backbone(inputs)\n        return self.classifier(x)","metadata":{"execution":{"iopub.status.busy":"2024-06-03T10:12:14.633429Z","iopub.execute_input":"2024-06-03T10:12:14.633827Z","iopub.status.idle":"2024-06-03T10:12:14.641779Z","shell.execute_reply.started":"2024-06-03T10:12:14.633793Z","shell.execute_reply":"2024-06-03T10:12:14.640822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# A model to exract features from the images based on a pretrained model","metadata":{}},{"cell_type":"code","source":"class FeatureExtractModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.backbone = timm.create_model(\n            CONFIG.BACKBONE,\n            pretrained=True,\n            num_classes=0,\n        )\n    \n    def forward(self, images):\n        image_features = self.backbone(images)\n        \n        return image_features","metadata":{"execution":{"iopub.status.busy":"2024-06-03T09:58:31.119937Z","iopub.execute_input":"2024-06-03T09:58:31.120289Z","iopub.status.idle":"2024-06-03T09:58:31.126688Z","shell.execute_reply.started":"2024-06-03T09:58:31.120264Z","shell.execute_reply":"2024-06-03T09:58:31.125466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = FreezeModel() #Model() # uncomment the model you want to use\nmodel = model.to('cuda')","metadata":{"execution":{"iopub.status.busy":"2024-06-03T10:12:17.262057Z","iopub.execute_input":"2024-06-03T10:12:17.262427Z","iopub.status.idle":"2024-06-03T10:12:21.401461Z","shell.execute_reply.started":"2024-06-03T10:12:17.262398Z","shell.execute_reply":"2024-06-03T10:12:21.400405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Set upthe various parameters for training","metadata":{}},{"cell_type":"code","source":"def get_lr_scheduler(optimizer):\n    return torch.optim.lr_scheduler.OneCycleLR(\n        optimizer=optimizer,\n        max_lr=CONFIG.LR_MAX,\n        total_steps=CONFIG.N_STEPS,\n        pct_start=0.1,\n        anneal_strategy='cos',\n        div_factor=1e1,\n        final_div_factor=1e1,\n    )\n\nclass AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val):\n        self.sum += val.sum()\n        self.count += val.numel()\n        self.avg = self.sum / self.count\n\nMAE = torchmetrics.regression.MeanAbsoluteError().to('cuda')\nR2 = torchmetrics.regression.R2Score(num_outputs=CONFIG.N_TARGETS, multioutput='uniform_average').to('cuda')\nLOSS = AverageMeter()\n\nY_MEAN = torch.tensor(y_train).mean(dim=0).to('cuda')\nEPS = torch.tensor([1e-6]).to('cuda')\n\ndef r2_loss(y_pred, y_true):\n    ss_res = torch.sum((y_true - y_pred)**2, dim=0)\n    ss_total = torch.sum((y_true - Y_MEAN)**2, dim=0)\n    ss_total = torch.maximum(ss_total, EPS)\n    r2 = torch.mean(ss_res / ss_total)\n    return r2\n\nLOSS_FN = nn.SmoothL1Loss() # r2_loss\n\noptimizer = torch.optim.AdamW(\n    params=model.parameters(),\n    lr=CONFIG.LR_MAX,\n    weight_decay=CONFIG.WEIGHT_DECAY,\n)\n\nLR_SCHEDULER = get_lr_scheduler(optimizer)","metadata":{"execution":{"iopub.status.busy":"2024-06-03T09:59:09.605302Z","iopub.execute_input":"2024-06-03T09:59:09.605697Z","iopub.status.idle":"2024-06-03T09:59:09.628688Z","shell.execute_reply.started":"2024-06-03T09:59:09.605667Z","shell.execute_reply":"2024-06-03T09:59:09.627619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Extract features from the images based on a model of choice","metadata":{}},{"cell_type":"code","source":"def extract_features(dataloader, model):\n    model.eval()\n    features_list = []\n    \n    with torch.no_grad():\n        for step, (X_batch, _) in enumerate(dataloader):\n            X_batch = X_batch.unsqueeze(0).to('cuda')\n            # get the features and bring them back to the cpu so we can use them \n            # to make the dataset\n            y_pred = model(X_batch).cpu()\n            features_list.extend(y_pred)\n    \n    features_array = np.array(features_list)\n    \n    # Convert the features array into a DataFrame\n    features_df = pd.DataFrame(features_array)\n    \n    features_df.columns = [f'feature_{i}' for i in range(features_array.shape[1])]\n    \n    return features_df\n\nif CONFIG.EXTRACT_FEATURES:\n    feature_extract_model = FeatureExtractModel()\n    feature_extract_model = model.to('cuda')\n\n    df = extract_features(train_dataloader, feature_extract_model)\n    df.to_pickle(\"swin_features.pkl\")\n    df = extract_features(test_dataset, feature_extract_model)\n    df.to_pickle(\"swin_features_test.pkl\")","metadata":{"execution":{"iopub.status.busy":"2024-06-03T09:59:13.032927Z","iopub.execute_input":"2024-06-03T09:59:13.033681Z","iopub.status.idle":"2024-06-03T09:59:13.040456Z","shell.execute_reply.started":"2024-06-03T09:59:13.033651Z","shell.execute_reply":"2024-06-03T09:59:13.039489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train the model","metadata":{}},{"cell_type":"code","source":"if CONFIG.TRAIN_MODEL is True:\n    print(\"Start Training:\")\n    for epoch in range(CONFIG.N_EPOCHS):\n        MAE.reset()\n        R2.reset()\n        LOSS.reset()\n        model.train()\n\n        for step, (X_batch, y_true) in enumerate(train_dataloader):\n            X_batch = X_batch.to('cuda')\n            y_true = y_true.to('cuda')\n            t_start = time.perf_counter_ns()\n            y_pred = model(X_batch)\n            loss = LOSS_FN(y_pred, y_true)\n            LOSS.update(loss)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n            LR_SCHEDULER.step()\n            MAE.update(y_pred, y_true)\n            R2.update(y_pred, y_true)\n\n            if not CONFIG.IS_INTERACTIVE and (step+1) == CONFIG.N_STEPS_PER_EPOCH:\n                print(\n                    f'EPOCH {epoch+1:02d}, {step+1:04d}/{CONFIG.N_STEPS_PER_EPOCH} | ' + \n                    f'loss: {LOSS.avg:.4f}, mae: {MAE.compute().item():.4f}, r2: {R2.compute().item():.4f}, ' +\n                    f'step: {(time.perf_counter_ns()-t_start)*1e-9:.3f}s, lr: {LR_SCHEDULER.get_last_lr()[0]:.2e}',\n                )\n            elif CONFIG.IS_INTERACTIVE:\n                print(\n                    f'\\rEPOCH {epoch+1:02d}, {step+1:04d}/{CONFIG.N_STEPS_PER_EPOCH} | ' + \n                    f'loss: {LOSS.avg:.4f}, mae: {MAE.compute().item():.4f}, r2: {R2.compute().item():.4f}, ' +\n                    f'step: {(time.perf_counter_ns()-t_start)*1e-9:.3f}s, lr: {LR_SCHEDULER.get_last_lr()[0]:.2e}',\n                    end='\\n' if (step + 1) == CONFIG.N_STEPS_PER_EPOCH else '', flush=True,\n                )\n\n    torch.save(model, 'model.pth')","metadata":{"execution":{"iopub.status.busy":"2024-06-03T09:59:15.757067Z","iopub.execute_input":"2024-06-03T09:59:15.757693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submit the results","metadata":{}},{"cell_type":"code","source":"SUBMISSION_ROWS = []\nmodel.eval()\n\nfor X_sample_test, test_id in tqdm(test_dataset):\n    with torch.no_grad():\n        y_pred = model(X_sample_test.unsqueeze(0).to('cuda')).detach().cpu().numpy()\n    \n    y_pred = SCALER.inverse_transform(y_pred).squeeze()\n    row = {'id': test_id}\n    \n    for k, v in zip(CONFIG.TARGET_COLUMNS, y_pred):\n        if k in LOG_FEATURES:\n            row[k.replace('_mean', '')] = 10 ** v\n        else:\n            row[k.replace('_mean', '')] = v\n\n    SUBMISSION_ROWS.append(row)\n    \nsubmission_df = pd.DataFrame(SUBMISSION_ROWS)\nsubmission_df.to_csv('submission.csv', index=False)\nprint(\"Submit!\")","metadata":{"execution":{"iopub.status.busy":"2024-05-30T15:52:25.780148Z","iopub.status.idle":"2024-05-30T15:52:25.780485Z","shell.execute_reply.started":"2024-05-30T15:52:25.780326Z","shell.execute_reply":"2024-05-30T15:52:25.780340Z"},"trusted":true},"execution_count":null,"outputs":[]}]}